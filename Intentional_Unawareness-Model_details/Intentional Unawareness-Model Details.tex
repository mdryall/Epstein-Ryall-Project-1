\documentclass[
%draft,
11pt,
titlepage,
reqno,
%	oneside,
%	twocolumn
]{article}%Draft option puts "slugs" in the margin for overfull lines

%\usepackage{newlattice}%custom package by Gratzer. Use with amsart See book for details.
%Packages loaded by amsart:
%\usepackage{amsmath}%This loads amsbsy, amsopn, amstext
%\usepackage{amsfonts}
\usepackage{amsthm}%This loads amsgen
\usepackage{amsxtra}
\usepackage{geometry}

%\usepackage{pdfsync}
%\usepackage{upref}
%\usepackage{amsidx}
%\usepackage{stmaryrd} %This adds small left arrows for accents that more closely mirror the \vec command
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{latexsym}
\usepackage{amsmath}

\usepackage{exscale}
\usepackage{amscd} %commutative diagrams
\usepackage{dcolumn} %to get decimal places aligned in tables
\usepackage{array}
\usepackage{tabularx}
%\usepackage{MnSymbol} %dashed arrows and more - see documentation
\usepackage[mathscr]{eucal}
\usepackage[english]{babel}
\usepackage[pdftex]{graphicx}
\usepackage{booktabs}%for nice tables (see discuss at https://people.inf.ethz.ch/markusp/teaching/guides/guide-tables.pdf). For details see https://ctan.org/pkg/booktabs.
% A FANCY TABLE
% \begin{table*}\centering
	% \ra{1.3}
	% \begin{tabular}{@{}rrrrcrrrcrrr@{}}\toprule
		% & \multicolumn{3}{c}{$w = 8$} & \phantom{abc}& \multicolumn{3}{c}{$w = 16$} &
		% \phantom{abc} & \multicolumn{3}{c}{$w = 32$}\\
		% \cmidrule{2-4} \cmidrule{6-8} \cmidrule{10-12}
		%     & $t=0$    & $t=1$    & $t=2$  & & $t=0$    & $t=1$    & $t=2$   & & $t=0$    & $t=1$   & $t=2$\\ \midrule
		% $dir=1$\\
		% $c$ & 0.0790   & 0.1692   & 0.2945 & & 0.3670   & 0.7187   & 3.1815  & & -1.0032  & -1.7104 & -21.7969\\
		% $c$ & -0.8651  & 50.0476  & 5.9384 & & -9.0714  & 297.0923 & 46.2143 & & 4.3590   & 34.5809 & 76.9167\\
		% $dir=0$\\
		% $c$ & 0.0357   & 1.2473   & 0.2119 & & 0.3593   & -0.2755  & 2.1764  & & -1.2998  & -3.8202 & -1.2784\\
		% $c$ & -17.9048 & -37.1111 & 8.8591 & & -30.7381 & -9.5952  & -3.0000 & & -11.1631 & -5.7108 & -15.6728\\
		% \bottomrule
		% \end{tabular}
	% \caption{Caption}
	% \end{table*}
\usepackage{subcaption}%for tables and such
\usepackage{lipsum} %for preventing breaks and such
%\usepackage{pgf,pgfarrows,pgfnodes,pgfshade}
\usepackage{setspace} %Turn ON for editing
%\usepackage{verbatim}
%\usepackage{enumerate}
%\usepackage{xspace}`
%\usepackage{longtable}
%\usepackage{epstopdf}
%\usepackage[authoryear]{natbib}
%\usepackage{lscape}
\usepackage{mathabx}
\usepackage{natbib}
\bibliographystyle{chicago}

%\theoremstyle{plain}
\newtheorem{acknowledgement}{Acknowledgement}
\newtheorem{assumption}{Assumption}
\newtheorem{axiom}{Axiom}
\newtheorem{case}{Case}
\newtheorem{claim}{Claim}
\newtheorem{conclusion}{Conclusion}
\newtheorem{condition}{Condition}
\newtheorem{conjecture}{Conjecture}
\newtheorem{corollary}{Corollary}
\newtheorem{criterion}{Criterion}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{econjecture}{Empirical Conjecture}
\newtheorem{example}{Example}
\newtheorem{exercise}{Exercise}
\newtheorem{lemma}{Lemma}
%\theoremstyle{remark}
\newtheorem{remark}{Remark}
\newtheorem*{notation}{Notation}
\newtheorem{proposition}{Proposition}
\newtheorem{theorem}{Theorem}
\newtheorem*{main}{Main Theorem}
\newtheorem{solution}{Solution}
\newtheorem{summary}{Summary}
%\newenvironment{proof}[1][Proof]{\noindent\textbf{#1.} }{\ \rule{0.5em}{0.5em}}

\newcommand{\BigFig}[1]{\parbox{12pt}{\Huge #1}}%See Gratzer l. 2442 (for matrix)
\newcommand{\BigZero}{\BigFig{0}}

\doublespacing %This is a command from the SetSpace package

\geometry{letterpaper}
\setlength{\oddsidemargin}{0in}
\setlength{\topmargin}{0in}
\setlength{\topskip}{0in}
\setlength{\headsep}{0in}
\setlength{\headheight}{0in}
\setlength{\textwidth}{6.5in}
\setlength{\textheight}{8.75in}

%junk comment for Git

\begin{document}
	
	\title{Intentional Awareness\thanks{}
	}
	\author
	{
		Brian Epstein \\Tufts University, Medford
		\and 
		Michael D.\ Ryall \\University of Toronto 
	}
	\date{\today}
	\maketitle
	
	%\begin{abstract}
	
	%\end{abstract}
	
	%\doublespacing
	\def\baselinestretch{1.5}\small\normalsize
	\newcommand{\ra}[1]{\renewcommand{\arraystretch}{#1}}%for tables
	\newpage
	
	
	
	\section{Introduction}\label{sec: intro}
	This note presents the full mathematical description of the intentional awareness model. 
	It is not meant to be a paper.
	Rather, it is a full elaboration of a model that can be summarized or referred to in a paper.
	Some discussion of how certain mathematical objects are intended to be interpreted is provided (though, these descriptions are not at the same level of detail required for a paper).
	
	\subsection{Overview}
	In what follows, we develop a four-phase model of intentional acts. 
	The essential aim of this formalism is to take seriously the cognitive constraints we face as finite, material beings.
	In particular, we proceed from the uncontroversial claim that, at any given moment, an individual can only  attend to some finite number of conscious concerns. 
	We say that an individual is \textit{aware} of the matters toward which his or her attention is directed.
	Under constrained awareness, intentions take on an important role that is distinct from beliefs and desires.
	
	Our approach refines some existing discussions on this topic by distinguishing between states and acts. 
	A state is a snapshot of the world at a given moment in time that describes the status of all the features that are relevant to the situation at hand. 
	An act is a procedure that unfolds over time. 
	Starting in a state of the world at time $t$, the willful acts of individuals and the brute acts of Nature jointly determine the state of the world at time $t+1$.
	This interaction is elaborated in the following sections.
	
	Acts include both efforts that are inherently invisible to others (i.e., mental activities such as deliberating, judging, and choosing) and those that are observable (e.g., enrolling in a graduate course).
	We refer to the latter as \textit{actions} to distinguish them from the sorts of acts that can only be observed by the acting individual. 
	Thus, actions are a subcategory of act.
	Because states of the world include cognitive attitudes, all forms of act have the power to influence future states of the world.
	
	An individual in our model proceeds from an initial state of the world at time $t$ to a future action according to the following sequence of phases. 
	Each phase is assumed to take \textit{at least} one unit of time. 
	During a phase, the individual and Nature may act, thereby bringing the world to a new state. Individuals recall their experiences from earlier phases in later phases.
	\begin{enumerate}
		\item \textbf{Problem Selection:} 
		Contemplating their awareness, beliefs, knowledge, and preferences as featured in the state at $t$, individuals identify the set of act-problems.
		An \textit{act-problem} is an opportunity for the decision maker to achieve a desired goal by influencing the evolution of future states through her acts. 
		The problem is to settle upon a plan by which to cause or contribute to the evolution of the world to a state in which the target goal is attained.
		\textit{	The output of this phase is the selection of an act-problem to solve}.
		Alternatively, the decision maker may choose to wait and evolve to a new state in which they may select another problem for consideration.
		
		The world evolves to a new state.
		\item \textbf{Deliberation:} 
		Contingent upon the act of orientation to engage in an act of deliberation and given the awareness, beliefs, knowledge, and preferences featured in this new state, individuals conduct an analysis to determine which goal should be pursued. 
		Individuals screen out infeasible and dominated goals and then rank-order the remaining ones according to their preferences. 
		\textit{The completion of this analysis is a conclusion about which goal to pursue}. 
		If no goal is best, revert to a new Problem Selection phase.
		
		The world evolves to a new state.
		
		\item \textbf{Judgment:} 
		Contingent upon the goal selected as best and given the awareness, beliefs, knowledge, and preferences featured in this new state, individuals decide whether to pursue the goal. 
		\textit{The output of this phase is a commitment to formulate a plan to achieve the goal}. 
		Failing  that, the individual reverts to a new Problem Selection phase.
		
		The world evolves to a new state.
		
		\item \textbf{Planning:} 
		Contingent upon the commitment to plan and given the awareness, beliefs, knowledge, and preferences featured in this new state, individuals  formulate a state-contingent plan of action. 
		This plan includes the goal in the support of their beliefs (i.e., individuals believe that if the plan is implemented the goal will occur with positive probability). 
		\textit{The output of this phase is a plan and a commitment to activate the plan}.
		Alternatively, the individual may revert to a  new Problem Selection phase.
		
		The world evolves to a new state.
		
		\item \textbf{Acting:} 
		Upon entering the new state, the individual checks her  awareness, beliefs, knowledge, and preferences, then: i) if the state is a contingency included in the plan, then take the action as proscribed; or ii) if not, revert to a new orientation phase.
		\textit{The output of this phase is an action}.
		Alternatively, the individual may revert to a  new Problem Selection phase.
		
		The world evolves to a new state.
	\end{enumerate}
	
	For comparison, Holdon's (p.\ 57) four phase characterization of a typical exercise of freedom of the will unfolds as follows: 
	\begin{enumerate}
		\item \textbf{Deliberating}: Considering the options that are available, and their likely consequences; getting clear on one’s own desires, and one’s own prior plans and intentions; seeing how the options ﬁt in with these desires and plans; establishing pros and cons. 
		\item \textbf{Judging} (deciding that): Making a judgment that a certain action is best, given the considerations raised in the process of deliberation. 
		The upshot of the judgment is a belief. 
		\item \textbf{Choosing} (deciding to): Deciding to do the action that one judged was best. 
		The upshot of this decision is an intention. 
		\item \textbf{Acting}: Acting on the intention that has been made, which involves both doing that thing, and coordinating other actions and intentions around it.
	\end{enumerate}
	
	\paragraph{Comments} The key differences between the two approaches are the following. 
	First, there is a distinction between states of the world and acts which flow over time. 
	Thus, we make explicit the idea that the world is changing as the decision maker tics through the phases. 
	Our first phase recognizes that an individual lands in a state and, at that point, must make some sense of the situation, exercising a certain degree of discretion in  organizing themselves for a deliberation. Holdon does not include such a phase. 
	Our second phase, Deliberation, follows Holdon fairly closely. 
	The main difference is that, in our case, the options are rank-ordered at the end of the phase but with no decision yet to advance to the planning stage. 
	In our Judgment phase, the decision is whether or not to move on to the planning phase (which may be influenced by the evolution of states). 
	Our Planning phase is like Holdon's Choosing phase in the sense that it constitutes the commitment to act. 
	This is because, barring winding up in a state that falls outside the scope of the plan, the individual acts according to plan (there is no reason not to). 
	The difference is that, in addition to the trivial case of simply choosing to do one action no matter what (as in Holdon's case), our plan can be dynamic and state-contingent. 
	Our Acting phases are pretty much identical. The one difference we have in mind is that all the phases are mutually exclusive in our setup except acting. 
	That is, a person can be doing Phase 5 from a previous decision sequence while engaging in a new decision sequence.  
	
	I suggest we combine Phases 2 and 3. Separating out the Judgment phase seems important to Holden. 
	But, I don't see what this adds and rolling in to Phase 2 puts us back to a 4-phase process. 
	Note also that awareness, beliefs, and knowledge are evolving in every one of our phases. 
	This seems to be in contradiction to Holden, where ``beliefs'' arise as a result of a judgment. 
	(I am not even sure how to interpret this, by the way.) 
	On the other hand, it is not clear where, exactly, in our approach, the ``intention'' appears. 
	Each move to a new phase involves a conscious decision to do so. 
	Hence, one could say, there are intentions at every step.
	
	The idea is as follows.
	To the extent some share of the mind's resources are occupied in solving a problem (e.g., deciding what kind of car to buy), those resources are not available for other conscious operations, such as solving other problems, constructing a feasible plan by which to acquire a car, or actualizing that plan by driving to the car dealer and making the transaction.
	We conjecture that an individual's finite stock of cognitive resources almost always acts as a hard constraint on his or her decision- and act-making capability.
	In our model, intentions serve as the pivot from goal choice assessment to goal acquisition planning and implementation.
	The formation of an intention moves an individual from a state of reckoning about what goal to pursue to one in which that choice becomes a commitment accompanied by \textit{plan} by which to attain it.
	Thus, forming an intention frees up the mental resources required to determine which goal to pursue and how to pursue it.
	When events arise consistent with the plan, the individual can proceed accordingly -- without engaging the mental machinery required to reassess goals and plans.
	Because deciding to focus attention on some new problem can, itself, be an intentional goal, one's awareness is dynamic and, to some extent, influenced by one's own intentions.
	As we will see, there are also social implications as individuals become aware of the intentions of others.  
	
	Beliefs and desires will operate in a familiar way. 
	The distinction here is that they are restricted to those matters about which an individual is aware.
	As we show below, because beliefs cannot account for awareness and because intentions shift awareness, a belief-desire model cannot do the work of an awareness-belief-desire-intention model.
	
	\subsection{Awareness}
	
	There are two conditions that must be met for an individual to be aware of some feature of the world. 
	\begin{enumerate}
		\item The feature must be accessible to the individual for active consideration. The sources of accessible features include contemporaneous sense data, imagination, and knowledge---essentially, anything toward which an individual can mindfully attend.
		\item An individual must choose to incorporate that feature into a conscious thought process.
	\end{enumerate}
	These conditions reflect our focus on decision making, as opposed to what mental phenomena might be going on when an agent sits idly thinking with no purpose in mind. 
	Because conscious capacity is finite, at any given moment an individual will be aware of a small subset of all the features constituting the state of the world.
	
	In some cases, a feature of the world may force itself into an individual's awareness through sense data, such as the pilot becoming aware of an alarm screeching in the cockpit.
	Even though the pilot does not control the breaking-through of the noise into his consciousness when the alarm goes off, at the point it does he then has a choice as to whether to incorporate the fact of the alarm sounding into his decision process or not.
	If for some reason the pilot should decide that the alarm is not relevant to any of his active decision deliberations, we count him as being unaware of it---even though it remains audible (and even irritating to listen to), it is not a factor in any deliberation he is presently undertaking.
	Alternatively, a feature of the world may be intentionally called to mind, such as when a pilot in mid-flight calls upon his knowledge of how to navigate the jetliner. 
	One cannot bring to mind aspects of the world that one does not know or cannot imagine, such as an airline pilot who has never been to medical school pondering the technical pros and cons of cutting-edge heart transplant procedures. 
	
	Central to our approach is the assumption that humans face extreme constraints in the number of features of the world to which they can mindfully attend in any given moment.
	Given these constraints and the fact that humans are constantly flooded with more information than they can effectively incorporate into any deliberation, we see that mental effort is required to keep relevant information in mind, as opposed to being required to banish unnecessary information from it.
	For example, unless the pilot chooses to maintain awareness of the cockpit alarm (presumably, because it is relevant to something he is trying to do), we are claiming that the fact of the alarm will automatically fade to unawareness as part of a natural process of the pilot's  cognitive architecture. 
	
	Although this strikes us as an uncontroversial position, it has non-trivial implications.
	In particular, an open issue in the philosophy of action is whether it is rational for individuals to make commitments to ignore new information which, properly considered, might cause them to change their future plans.
	From our perspective, ``ignoring'' information---in the sense of being unaware of it---is the baseline state of most information accessible for human cognition.
	Given that an individual has the mental capacity to focus upon only a tiny fraction of the world's features at any one time, the question for rationality is not whether one should reflect upon the set of all relevant information and then decide whether to brush some of it aside.
	Rather, it is to determine which one of the multitude of issues that are rendered mutually exclusive for the purpose of reflection (due to cognitional constraints) should be brought to into active consideration (at the expense of the benefits available from reflecting upon some other things instead).
	
	Awareness and unawareness have long been a tricky problem for decision theorists. 
	%A decision maker can only choose among the potential acts about which she is aware. 
	%Moreover, in addition to uncertainty about the future consequences of an act, the decision problem may be confounded by unawareness of information relevant to the situation at hand. 
	%The problem is how to model what happens in a dynamic setting when the decision maker suddenly faces an unexpected consequence. 
	For example, in a standard Bayesian decision problem, unawareness of certain consequences could be modeled as zero-probability states according to the decision maker's subjective beliefs. 
	However, Bayesian decision makers will be confounded should a subjectively impossible state occur. 
	What then?
	Added to this is the problem of representing interactive decision makers with different states of awareness (i.e., when the acts of one affects the consequences of the acts of the others). 
	
	\citet{Dekel1998} demonstrate that standard state-space approaches cannot model unawareness. 
	\citet{Schipper2015} surveys various alternatives to modeling unawareness, including approaches from  AI, logic, and game theory. 
	We adopt a version of the framework developed in \citet{Heifetz2006} \citep[also see][for related extensions]{Heifetz2008,Heifetz2013} that is both simpler, in the sense that we focus upon a single-agent decision problem, and extended, in the sense that an agent's space of awareness may vary.
	
	
\section{Notational conventions}

Capital letters ($G$, $N$, etc.) refer to sets and to set-valued correspondences.  
Small Arabic and Greek letters refer variously to elements of sets (e.g., $i\in N$) and functions (e.g., $\sigma:N\rightarrow \mathcal{N}$). 
Terms  are \textit{italicized} at the point of definition.  
A \textit{profile} is a placeholder for a list of elements.
We denote these in boldface: e.g., $\mathbf{x}$ where $\mathbf{x}\equiv(x_1,\ldots,x_n)$. 
The ``$\equiv$'' symbol indicates the definition of a mathematical object. 
If $X$ is a set, then $2^X$ denotes the set of all subsets of $X$. Calligraphic letters refer to sets of sets (e.g. $\mathcal{X}\equiv 2^X$). 
Curly parentheses indicate sets, typically in defining them (e.g. $X\equiv\{x|x\text{ is an even integer}\}$). 
The notation ``$|\cdot|$'' indicates set cardinality (e.g., if $X\equiv\{a,b,c\}$, then $|X|=3$). 
If $X$ is a set and $Y\subset X$, then $X\setminus Y$ is the set $X$ minus $Y$; i.e., the set of elements of $X$ that  remain when the elements of $Y$ are removed. 
All sets are assumed to be finite unless otherwise indicated.\footnote
{
	In almost all cases, our results extend to uncountably infinite sets (e.g., the domains and ranges of continuous variables). However, extending the analysis to include these would involve bulking up the discussion with technical material that would add little, if anything, to the conceptual content of the model.
}
	
	
\section{Objective Reality}\label{sec:world}
	
In this section, we describe the status and dynamics of reality---the world as it actually is, could have been, or might yet be along with the causes that drive it to unfold in a particular way. 
Once we describe the way the world works here, we move on to the next section to describe the way a single decision maker understands and thinks about it.
In this setup, there are two \textit{actors}, a human decision maker and Nature, labeled $i$ and $n$, respectively.\footnote
{
	In future work, we will investigate interactions between agents and group decisions.
		This model lays the foundation for those extensions.
} 
Nature is included to account for  a God's-eye view of the status of the world in all its richness as well as for the phenomena that occur outside the decision maker's acts that, jointly with them, determine the instantiations of the world through time.
We focus on the action over a fixed period, from $t=0$ to $t=T$. 
Time is indicated with subscripts.
	
%We attach ``$\oplus$'' superscripts mathematical objects to indicate they correspond to objective reality.
%The ``$s$'' superscript is used to indicate an object corresponds to the perceptions of the individual.
%In general, $\oplus$-objects are richer and more refined than $s$-objects. 
%We use $n$ and $i$ superscripts to indicate that an object belongs to or is chosen by Nature ($n$) or the individual ($i$).
	
	
	
	
\subsection{States of the world\label{sec:states}}
	
As outlined in the Introduction, at time $t$, individual $i$ finds himself in a particular situation, which we term an \textit{objective state of the world}.
This state corresponds to objective reality, including the status \textit{of all features of the world} in that moment. 
Importantly, these include both the  mind-independent features of the world as well as the \textit{mental attitude} of the individual acting in that world. 
Let $S\equiv\{s_0,\ldots,s_m\}$ denote the (indexed) \textit{objective state space}, with typical element, $s_k$, a \textit{state}.
We drop state indices when they are not needed to reduce notational clutter.

$S_t\subset S$ is the subset of states  that can could be actualized at time $t$, where $t=0,\ldots,T$ and $T>0$.
If $t$ is the present or some historic time, then \textit{one}  $s_k\in S_t$ is factual and the others are counterfactual. 
If $t$ is some future time, then $S_t$ elaborates all the potential states that could be actualized at $t$.

It does no harm to imagine that each state elaborates an uncountably infinite number of features of the world.
Even so,  we assume that the the number of states in $S$ is finite.
The rationale for this is two-fold.
First, the number of features of the world that are relevant to an individual decision maker in a specific state is often finite (though, possibly quite large).
Moreover, the number of possible instantiations of each feature may be finite or effectively approximated by a finite number of  categories (e.g., profits in dollars or temperature ranges).
If so, then the number of states required to elaborate all the possibilities is also finite.
Second, even  though we lose a measure of generality by making this assumption, doing so eliminates a substantial amount of mathematical complexity that, were it included to account for an uncountably infinite number of states, would add little in the way of philosophical insight.
	
	
	
\subsection{Acts}\label{sec:acts}
	
In our approach the acts of Nature and the decision maker jointly cause the system to evolve from a state $s_k\in S_t$ to a new state $s_l\in S_{t+1}$.
Acts of Nature represent all the causes that, in conjunction with the act of the individual, co-determine the actualization of a particular state from an immediately preceding, previously actualized state.
	

For each  $s_k\in S$, let $A^j_k$ indicate the set of \textit{feasible acts available to actor $j$ in state $s$} with arbitrary element $a^j_k\in A^j_k$, where $j\in\{i,n\}$.
The set of feasible acts available in a state is a feature of that state.\footnote
{
	Because we consider the intentional formation of some mental attitudes as choices available to individuals, we use the term ``act'' to describe the choices available to someone in a broad way.
	We think of ``action'' as describing the narrower category of act associated with physical movement.
} 
We adopt the convention that $A^j_k=\emptyset$ indicates that actor $j$ has no feasible acts in state $s$.
An \textit{objective act profile at $s$}  is a pair of acts, $a_k\equiv(a^i_k,a^n_k)$, one by Nature and one by the individual, respectively. 
The set of \textit{all objective act profiles at state $s$} is $A_k\equiv A^i_k\times A^n_k$.
Note the implication that the acts of $i$ and $n$ at $s_k$ are simultaneous---that is, while $i$ is in the process of acting, there are other things going on in the world that may also have an impact on the features of the world of interest to $i$.
Because they are sets, the acts in each $A^j_k$ are unique.
This implies the act profiles in each $A_k$ are also unique.
The set of \textit{all possible objective act profiles at time $t$} is  $A_t\equiv \bigcup_{s_k\in S_t} A_k$; and the set of \textit{all possible objective act profiles} is $A\equiv \bigcup_{s_k\in S} A_k$.
The uniqueness of act profiles in each $A_k$ does not imply the same for $A_t$ or $A$: an actor may have the same feasible acts in different states.

 
	
\subsection{Dynamics} 
	
As indicated above, the act profiles summarize all the conditions required to actualize one state from the previously actualized state. 	It may be helpful to think of act profiles as the ``flow'' variables between states---the activities that occur over a unit of time that cause the world to move from one state to the next.
	
To formalize this,  define the \textit{dynamic objective state graph},  $\Gamma\equiv( S,\precdot)$ which elaborates of all objectively possible sequences of states.
Specifically, assume $\Gamma$ is a directed, rooted tree with nodes $S$ ordered by the precedence relation $\precdot$; i.e., the predecessors of each $s\in S$ are totally ordered by $\precdot$. 
Thus, $S_0=\{s_0\}$, where $s_0$ is the root node at the beginning of time.
Let $\omega:A\rightarrow S$ be a function mapping act profiles available at a given state to its children.	
Thus, if $s_k\precdot s_{l}$, then $\omega(a_{k})=s_{l}$ means  $a_{k}$ is the act profile that causes the world to evolve from $s_k$ to $s_{s_{l}}$.
Assume $\omega$ is bijective from $A_k$ to the immediate successors of $s_k$ (the elements in $A_k$ uniquely label the edges from $s_k$ to its successors, one-to-one with no extras or shortfalls).
This means there is no state in which a specific act profile can lead from one state to more than one successor state.\footnote
{ 
	Two or more states may yet have the same the set of feasible act profiles. That is, for $s_k\ne s_l$ it it is possible that $A_k=A_{l}$. For example, at 11am $i$ may be finished with  work and get a cup of coffee or, alternatively, may not be finished with work and, yet, still have the option to get a cup of coffee. However, it is never the case---holding Nature's acts constant---that getting a cup of coffee in one state has an ambiguous effect on the future.
}
Because the relationship is bijective, we can also identify the acts leading to states; i.e., $\omega^{-1}(s_{l})=a_{k}$.

At the beginning of time, in state $s_0$, $i$ must choose one from  a number of mutually exclusive act-problems.
In future periods, $i$ is generally free to continue working on the current problem or abandon it and take up another.  
To refer to \textit{the act-problems available in an arbitrary state, $s_k$}, we define $P_k\equiv \{p_1,\ldots,p_{v_k}\}$, where $v_k$ is the number of act-problems available in $s_k$.\footnote
{
	In later periods, $t>0$, $A^i_k$ may contain numerous acts in addition to choosing a problem from $P_k$. Hence, the distinction is helpful.
}
In $s_0$, however, $i$'s only feasible act is to choose a problem to get started upon.
Therefore, $A^i_{0}=P_0$ is the indexed set of $v_0$ act-problems objectively available to $i$ in state $s_0$.
	
For each state $s_k$, let $h_k=(s_0,\ldots,s_k)$ denote the \textit{history at} $s_k$; i.e., the unique path from $s_0$ to $s_k$ in $\Gamma$.
Because $\omega$ is bijective on $A_k$, which contains no duplicates, each history $h_k$ is also associated with the unique sequence of act profiles that actualize it.
Abusing notation a bit, let this be represented by  $\omega^{-1}(h_k)\equiv(\omega^{-1}(s_0),\ldots,\omega^{-1}(s_k))$.

Let $H_T$ denote the set of  \textit{objective terminal histories}; i.e., of the form $h_s=(s_0,\ldots,s_k)$ where $s_k\in S_T$.
Note that the relationship between $H_T$ and $S_T$ is bijective: every terminal state corresponds to a unique history in $\Gamma$.
It is not difficult to show that each $\sigma\in\Sigma$ implies a probability distribution on the set of terminal histories.
We will abuse notation some more and write $\sigma(h)$ to be the probability of terminal history $h$ implied by $\sigma$.
		
\paragraph{Example: Brian's Infant, objective reality}
{\em	
We consider the problem of Brian's Infant, an extended example that we will use  to illustrate the formalism as we develop it.
The situation is as follows.
Brian's child, $i$, finds herself in $t=0$ presented with two, mutually potential act-problems: to choose a toy with which to play or to choose a TV show to watch. 
Let the toys be labeled $A$ and $B$. 
One of these toys is better than the other. 
Let $A^\ast$ indicate that $A$ is best and $B^\ast$ indicate that $B$ is best. 
Alternatively, the child can choose to watch TV show $W$ or $C$.
One of these shows is better than the other. 
Let $W^\ast$ indicate that $W$ is best and $C^\ast$ indicate that $C$ is best.


\begin{figure}[h!]
	\centering
	\includegraphics*[page=1,trim = 0in 1in 0in 0in,scale=.6]{Awareness_Diagrams_All}
	\caption{Evolution of Nature's state spaces from $t=0$ to $t=1$\label{Diag: p-01}}%trim: L B R TxPer
\end{figure}

We have $P_{0}=\{p_1,p_2\}$, where $p_1$ is the choose-a-toy problem and $p_2$ is the choose-a-show problem.
Each act-problem presents an actor with a distinct choice deliberation that occupies their full awareness capacity.
The idea is that cognitive constraints render solving the problems in $P_{0}$ mutually exclusive.
Here, $i$'s only feasible acts in state $s_0$ are  $A^i_{0}=\{p_1,p_2\}$.
At the same time, Nature determines which toy and which show are best.
Therefore, Nature selects one of among four possible acts: $A^n_{0}=\{(A^\ast,W^\ast),(A^\ast,C^\ast),(B^\ast,W^\ast)$, $(B^\ast,C^\ast)\}$.\footnote
{
	When $a^j_k$ is a compound act, as is the case with Nature in this example, we use parentheses to list the individual elements of the act.
}

Figure \ref{Diag: p-01} illustrates the evolution of the system from $t=0$ to $t=1$. 
From $s_0$, the objective action profiles are of the form $a_{0}=((Y,X),Z)$ where $a^n_{0}=(Y,X)$ is Nature's choice of  $Y$, the best toy, and $X$, the best show, and $a^i_{0}=Z$ is $i$'s choice of which act-problem to solve.
Then, $S_1$ contains eight possible states (one for each combination of these variables, as shown).

For example, $s_{3}=(B^\ast,W^\ast,p_1)\in S_1$ is state number 3 in period 1.
In this state: $B$ is the best toy; $W$ is the best show; and $i$ has decided to focus on choosing the toy with which to play.
This state is actualized by the act profile $a_0=((B^\ast,W^\ast),p_1)$.
The mapping function with respect to this state is $\omega((B^\ast,W^\ast),p_1)=s_{3}$.
The history associated with this state is $h_{3}=(s_0,s_{3})$.
This history corresponds to an act profile  sequence that includes a single element:  $\omega^{-1}(h_{s_{3}})=(a_0)$ where $a_0=((B^\ast,W^\ast),p_1)$.
}	
	
\subsection{Events}

The term `event' is used differently in philosophy than it is in probability theory. 
Since we are writing to audiences familiar with one or the other, it is important to clarify this difference. 
In probability theory, `event' is used similarly to the term `property' in philosophy, where properties are understood intensionally. 
Philosophers typically use `event' to mean a spatiotemporal particular extended over time. 
We refer to events associated with states at a moment in time (the probability theory usage) as \textit{synchronic events}, and those associated with properties extended through time (the philosophy usage) as \textit{diachronic events}.

We define synchronic events as subsets of a state space at a moment in time. 
For example, the event ``Mike intends to get a cup of coffee at $t$,'' includes \textit{all} the states in $S_t$ in which getting a cup of coffee is the intention of Mike. 
In philosophical terminology, this is equivalent to the property \textit{being in a state in which Mike intends to get a cup of coffee}, where the intension of the property is all the states of the world in which the world exemplifies that property.

In the Brian's child example in Figure \ref{Diag: p-01}, the synchronic event ``$A$ is the best toy,'' is the subset $\{s_{1},s_{2},s_{5},s_{6}\}\subset S_1$.
Alternatively, the event, ``$i$ is solving $p_1$," is $\{s_{1},s_{2},s_{3},s_{4}\}$.
Thus, if $i$ is aware of all the states in $S_1$ and knows that ``$A$ is the best toy,'' and that she is solving $p_1$, then she knows that one of $\{s_{1},s_{2}\}$ is the actual state of the world.

Diachronic events are defined as subsets of the set of terminal histories, $H_T$.
These identify to sequences of events over time and, implicitly, the sequences of action profiles that cause them.
For example, the set of terminal histories in the Brian's child example is
\[
	H=\{(s_0,s_{1}),\ldots,(s_0,s_{8})\}.
\]
The diachronic event, ``$i$'s period 0 act is $a^i_0=p_1$,'' is given by the subset
\[
	\{(s_0,s_{1}),(s_0,s_{2}),(s_0,s_{3}),(s_0,s_{4})\}\subset H_T.
\] 
Sometimes we refer to the ``path of actualization,'' to mean a history that is actualized (e.g., the red-highighted path in Figure \ref{Diag: p-01}).
Diachronic events that will be of interest are individual acts, sequences of act profiles, and features of the world that persist through time---all of which are associated with sequences of states which, themselves, are associated with subsets of $H$.
For example, the diachronic event \textit{state $s$ occurred} is the set of terminal histories that include state $s$ as a component.

It is worth noting that synchronic events imply diachronic events.
For example, the synchronic event, ``$A$ is the best toy in $t=1$,'' corresponds to the diachronic event that includes all histories in which this is true.\footnote
{
	A synchronic event is a subset of states at some time $t$. Its diachronic counterpart is the subset of all terminal histories that pass through those states.
}

\subsection{Summing up reality}
Pulling together the key parts of the formalism, mind-independent reality is described by a pair $(\Gamma,\omega)$.
These elaborate the objective state space, $S$, and their dynamic interrelationships, $\precdot$, as specified by the dynamic objective state graph $\Gamma$  and the mapping from act profiles (which are features of the states), $A$, to edges in the dynamic tree according to $\omega$.
  
	
\section{Awareness}
We begin to structure the individual's mental attributes by assuming that---in each objective state of the world---the individual has in mind a subjective version of $(\Gamma,\omega)$ plus some additional cognitional components that allow him to navigate through act problems. 
Specifically,for each $s_k\in S$,  let $(\Gamma^{k},\omega^{k})$ be $i$'s mental representation of $(\Gamma,\omega)$.\footnote
{
	Our notational convention going forward is that superscripts correspond to the index number of an objective state. 
}
Each element in $(\Gamma^{k},\omega^{k})$ represents $i$'s subjective awareness of the corresponding element in $(\Gamma,\omega)$.
In particular, the elements of \textit{awareness state space} $S^k$ (as specified by $\Gamma^{s_k}$) are subjective states that describe the features of the world as $i$ is aware them when $i$ is in $s_k$.
These include $i$'s awareness of the world's features in past actualized states, the present state, counterfactuals to these, and future states.
To avoid additional complexities for now, we assume that $i$ does not reflect on his awareness\footnote
{ 
	That is, in this analysis we do not raise the issue of awareness of awareness. 
	This will be an important issue in the multi-agent case, but we sidestep it for now.
}
Thus, like $S$, $S^k$ is partitioned by the subsets of states corresponding to each time period, $S^k_t$.

The awareness graph $\Gamma^k$ describes the individual's subjective organization of reality when he finds himself in objective state $s_k$. 
This construct is part of the reality elaborated by  $s_k$.
This raises the central question of how (ontological) reality, as described by $\Gamma$, corresponds to $i$'s (epistemological) perception of it in a given state, as described by $\Gamma^k$.
What we assume is that $i$ is aware of a limited but accurate version of reality: although $i$ may not be aware of all its features,  those of which he \textit{is} aware are correct. 

To formalize this, for each objective state $s_k\in S$, define the surjective \textit{state-$s_k$ awareness mapping} $r^k:S\rightarrow S^k$ where $s^{k}_w=r^k(s_l)$ means that---when $i$ is in objective state $s_k\in S$---the (subjective) \textit{awareness state} $s^{k}_w\in S^k$ describes the features  of the world of which he is aware with respect to some objective state $s_l\in S$ (it may be that  $s_l=s_k$).
For any time $t$, we assume that $r^k$ is surjective (onto) from $S_t$ to $S^k_t$.
This means that the elements of $S^k_t$ typically represent a coarsening of reality and never a refinement of it. 
For a given awareness graph, $\Gamma^k$, and objective states $s_j,s_l\in S$, $s^{k}_w=r^{k}(s_j)$ and $s^{k}_w=r^k(s_l)$ means that, when $i$ is in state $s_k$, the statuses of the features of the worlds of which he is aware in states $s_j$ and $_l$ are identical.

\paragraph{Example} 
\emph{
	To see the idea, suppose $s_k\in S_t$ is a state in which $i$'s coffee is too hot to drink and this is the only feature of reality of which $i$ is aware.
Presumably, $S_t$ is huge, with each state capturing features of the world beyond the temperature of the coffee---such as the weather, where $i$ is geographically located, what is on TV, the level of gas in the car, and so on, many of which feature coffee that is too hot to drink.
Then, $r^k$ projects all the states in which the coffee is too hot to drink into a subjective awareness state, say $s^{k}_w\in S^k_t$. 
Now, suppose $i$ is also be able to reason about (is aware of) a state in which the coffee is not too hot to drink.
Then, $r^k$ would project all the other states into a counterfactual subjective awareness state, say $s^{k}_x\in S^k_t$, in which this is true.
Because $r^k$ is surjective when restricted to $S_t$, the inverse $r^{k^{-1}}$ exists on this part of its domain: e.g., $r^{k^{-1}}(s^{k}_w)\subset S_t$ is the objective synchronic event, ``the coffee is too hot to drink.'' 
Suppose $s_l\in S_t$ is an objective state in which the coffee is not too hot to drink. 
Then, $s^{k}_x=r^k(s_l)$.
}

Here,   $s^{k}_w=r^{k}(s_k)$ means that $s^{k}_w$ elaborates only the features of the world of which $i$ is aware given the actualization of objective state $s_k$; it is what $i$ is aware of regarding his current state when that state is $s_k$.
Here, it is important to point out that each $\Gamma^k$ is considered to be a distinct mental (and mathematical) object.
Hence, $s^0_0=r^{0}(s_0)$ emphasizes that the features of which $i$ is aware when he is in objective state $s_0$ are captured by the awareness state $s^0_0$.

For now, we do not consider awareness hierarchies; i.e., situations in which $i$ is aware of $(\Gamma^k,\omega^k)$ and $i$ is aware that he is aware of $(\Gamma^k,\omega^k)$, etc.
Thus, $i$ is never perfectly aware of the status of \textit{all} features of the world since---at least for the situations we have in mind---he is not required to ponder his own mental states.
This means that, technically, we must provide different labels for objective states and their awareness counterparts (because the two are never identical); e.g., if $s^{0}_0=r^{0}(s_0)$, it is never the case that $s^{0}_0=s_0$.
That said, awareness states in different awareness graphs can be labeled identically if the features of the world of which $i$ is aware are the same.

Formally, consider two distinct awareness graphs $\Gamma^k$ and $\Gamma^{l}$ and some state $s_{j}\in S$ and the awareness projections of $s_j$ which are given by $s^{k}_w=r^{k}(s_j)$ and $s^{l}_x=r^{l}(s_j)$.
Then, writing $s^{k}_w=s^{l}_x$ means that $i$'s awareness of $s_j$ is identical in $s_k$ and $s_l$.


\paragraph{Example}
\emph
{
	For example, suppose $s_k,s_l\in S$ are objective states in which it is raining and sunny outside, respectively.
Further, assume that in both states,  $i$ is aware of a cat meme on his computer, but not the weather. 
Then, $s^{k}_w=r^k(s_k)$ is the subjective state in which $i$ is aware only of a cat meme on his computer.
It is also the case that $s^{k}_w=r^k(s_l)$: being in state $s_k$, $i$ cannot distinguish his present actualized  state in which it is raining from the counterfactual state in which it is sunny---the weather is simply not on his mind.
In the fully-elaborated objective world, $r^{k^{-1}}(s^{k}_w)$ is the synchronic event ``$i$ is aware of a cat meme''---i.e., all the states in which this is true.
Now, in the present state $s_k$, $i$ may also be aware that, tomorrow, he will be engrossed by a new meme---which must correspond to a different subjective state, say $s^{k}_x\in S^k_{t+1}$.
}
 
At this point, we have said: i) the collection of states in the individual's awareness graph correspond to a partition of the objective state space; and ii)  this correspondence maintains time consistency in the sense that real world states at $t$ never map to awareness states in some other period. We have said nothing about consistency with respect to feasible acts or dynamic consistency. 

The structure of feasible act profiles at awareness state $s^k_w\in S^k$ is assumed to be $A^k_{w}\equiv A^{k.n}_{w}\times A^{k.i}_{w}$, where $A^{k.j}_{w}$ is the set of feasible acts available to actor $j$ at $s^k_w$ of which $i$ is aware when $i$ is in objective state $s_k$.
The product structure implies \textit{Act Independence}:  the set of act profiles includes \textit{all combinations} of the feasible acts for $n$ and $i$ at $s^k_w$ of which $i$ is aware. 
As in the objective case, for each $s^k_w\in S^k$, we require $\omega^k$ to be bijective (one-to-one and onto) from $A^k_{w}$ to the immediate successors of $s^k_w$.

We impose two consistency conditions on the relationship between $A^k$ and $A$.
First,  if $s^k_w=r^k(s)$, then $A^{k.i}_{w}= A^i_k$.
In other words, when $i$ is in objective state $s_k$, he is aware of all his feasible acts.\footnote
{
	This requirement can be loosened to consider situations in which an individual is not aware of all the acts available to him. For now, we sidestep this issue.
}
Note that this correspondence is only imposed on the actualized state in which $i$ finds himself.
Second, if $s^k_w=r^k(s_k)$ then, in the sequence of act profiles $\omega^{k^{-1}}(h^k(s^k_w))$, the $i$-components correspond to reality.
In other words, along the actualized history, $i$  does not forget nor is he uncertain about his own acts leading to his present state (see the discussion in the Uncertainty section).
He may be unaware and/or uncertain about Nature's acts.
He may also label these according to some naming scheme of his own.
Here, $a^{k.n}_{w}=\emptyset$ means that $i$ is unaware of the move taken by Nature.

Outside of the preceding restrictions on act profile names and mapping, $i$ is free to label edges in $\Gamma^k$ as he wishes.
In many cases, edges in his awareness space will be an amalgam of edges in reality. 
Outside of his historical acts and those that are presently feasible to him, the names $i$ assigns to other (i.e., counterfactual or future)  edges in $\Gamma^s$ is not crucial.
The mapping function $r$ is the central determinant of the relationship between reality and $i$'s perception of it (i.e., between  $\Gamma$ and $\Gamma^k$, respectively). 
With this in mind, we impose the following consistency conditions on $r^k$:
\begin{enumerate}
	\item Dynamic consistency: for $s_j,s_k,s_l\in S$ such that $s_j\precdot s_l $, if $s^k_w=r^k(s_j)$ and $s^k_x=r^k(s_l)$, then $s^k_w\precdot^k s^k_x$ in $\Gamma^k$.
	\item No contemplation of future unawareness: for $s_k\in S$, if $s^k_w=r^k(s_k)$, then the subgraph in $\Gamma^k$ starting at $s^k_w$ is a tree rooted rooted at $s^k_w$.
	\item Awareness of counterfactuals:  for $s_k\in S_t$, $t>0$, if $s^k_w=r^k(s_k)\in S^k_t$, then there exists at least one $s^k_x\in S^k_t$ such that  $s^k_w\ne s^k_x$.
\end{enumerate}
Item 1 is important as it implies that terminal histories in the real world correspond to histories in the awareness graph (i.e., as $i$ is thinking about them).
Thus, histories in $H^k_T$ correspond to diachronic events in $H_T$.
Item 2 is intuitive in the sense that, as $i$ look looks to the future while contemplating how he will solve his  act-problem, he does not envision becoming unaware of what he is doing as he travels along one of his potential paths.
Item 3 says that, whatever $i$ is aware of with respect to his actualized state (at any period beyond the the start of time), he is also aware that things could have turned out differently (even if only in the very rough sense of, ``there could have been a state of the world that differs from this one in various ways'').

An important feature of awareness graphs is that they need not be trees. 
As $i$ moves forward in time, he is always aware of the actualized path that has brought him to his present state.
However, his awareness of future possibilities along the feasible path of actualization tends to become more refined as he proceeds from act-problem selection to action while, at the same time, his awareness of counterfactual details tends to fade into unawareness (with the recall caveats described above and in the section on mental attitudes).

Indeed, one interesting possibility is to impose some hard constraints on awareness graphs and actions. 
For example, although arbitrary, we could analyze individuals capable of: i) 10 awareness histories in any $\Gamma^k$; and ii) the ability to choose one act per period while solving an act-problem (i.e., those associated with phases I through IV  in Figure \ref{Diag: p-09}) \textit{plus} the ability to take action simultaneously according to an established plan (discussed below).
The constraints so imposed would, while arbitrary, allow us to analyze intentional unawareness in the sense of an individual  trying to manage his constrained cognitional resources in an optimal fashion.
	
\paragraph{Example: Brian's Infant, Part 2}	
{\em
Picking up the example of Brian's Infant where we left off, let consider the infant's awareness of reality.
Suppose the act profile is $a_{s_0}=((B^\ast,W^\ast),p_1)$.
According to Figure \ref{Diag: p-01}, this brings the world to objective state $s_{3}=((B^\ast,W^\ast),p_1)$ in  $t=1$.
Assume that, having decided to solve the toy-choice problem, $i$ becomes aware of which toy is best.
In this state, $i$ can reason about $B$ being the best toy and the counterfactual that $A$ could have been the best toy.
Here, we see that $i$ is also aware that she could have chosen to solve $p_2$, which would have put her in a different state of the world. 
Then, $S^{0}_0=\{s^0_0\}$, such that $P^0_0=P_0$, and  $S^{3}_1=\{(A^\ast,p_1),(B^\ast,p_1),(p_2)\}$.

In this state, her awareness tree is $\Gamma^{3}$, as depicted in Figure \ref{Diag: p-02}.
In this diagram, the top half shows $\Gamma$ tipped on its side. 
The bottom half illustrates $\Gamma^{3}$.
The objects associated with $i$'s mental states are illustrated in blue.
The actualized histories, in both $\Gamma$ and $\Gamma^{3}$ are shown in bold.

There are nine awareness state mappings (dashed lines with arrows), two of which are labeled.
For example, $i$ presently finds herself in $r^{3}(s_{3})=(B^\ast)$.
Notice that ${r^{3}}^{-1}(B^\ast)=\{s_{3},s_{4}\}$, which corresponds to the objective event, ``$B$ is the best toy and $i$ chooses to solve $p_1$.

The central take-away from Figure \ref{Diag: p-02} is that $i$ is completely unaware of the TV show choices.
In particular, it is \textit{not} the case that $i$ is \textit{uncertain} about the best TV show.
Rather, according to $\Gamma^{3}$, $i$ is simply not thinking about TV shows at all---they are not in her field of awareness and, therefore, are not a available for her deliberations between $t=1$ and $t=2$.

We can also check the consistency conditions on the awareness graph. 
Because the graph is truncated at $(B^\ast,p_1)$, we have not illustrated $i$'s feasible acts there. 
So, we cannot check that the feasible acts of which he is aware are, in fact, truly feasible acts as we require.\footnote
{
	We will illustrate this condition momentarily. 
}
We do see that, along the actualized history, $i$ is aware of his own choice of $p_1$.
Dynamic consistency is met because each objective history $\Gamma$ maps to a history in $\Gamma^{3}$.
No contemplation of future unawareness is not relevant in this diagram, again because $i$'s awareness history is truncated at $(B^\ast,p_1)$ (there is no future being contemplated in this diagram). 
Clearly, $i$ is aware of counterfactuals:  $(B^\ast,p_1)$ is not the only awareness state in $S^{3}_1$.

\begin{figure}[h!]
	\centering
	\includegraphics*[page=2,trim = 0in 0in 0in 0in,scale=.6]{Awareness_Diagrams_All}
	\caption{Awareness tree for Brian's Infant in objective state $s_{3}=(B^\ast,W,p_1)$.\label{Diag: p-02}}%trim: L B R TxPer
\end{figure}
}

	
\section{Mental attitudes}
	
\subsection{Uncertainty}
	
In addition to the states about which $i$ is aware, we wish to account for uncertainty. 
Uncertainty is not the same as unawareness.
For example, in the Brian's Infant example, $i$ could be aware that $A^\ast$ and $B^\ast$ are possible yet harbor uncertainty about which is consistent with the true state of the world---and all the while be utterly unaware of the weather.

\subsubsection{Information sets}
For each objective state $s_k\in S$, define the\textit{ information set at awareness state} $s^k_w\in S^k$, denoted $I^k_w$,  to be a subset of $S^k$.  
Information sets serve to distinguish states about which $i$ is aware but uncertain. 
Information sets are assumed to meet the following conditions:	
\begin{enumerate}
	\setcounter{enumi}{3}
	\item No-delusion: $s^k_w\in I^k_w$,
	\item Introspection: if $s^k_x\in I^k_w$, then $I^k_w=I^k_x$,
	\item Feasible act consistency: For all $s^k_w,s^k_x\in S^k$, if $s^k_x\in I^k_w$ then $A^{k.i}_w= A^{k.i}_x$,
	%\item Disjoint feasible act sets: if $s^\prime, s^{\prime\prime}\in S^s_t$, $s^\prime\ne s^{\prime\prime}$ and ${s^\prime}\in I^s_{s^{\prime\prime}}$, then $I^s_{s^\prime}=I^s_{s^{\prime\prime}}$.
	\item Time consistency: if $s^k_x\in I^k_w$ and $s^k_w\in S^k_t$, then $s^k_x\in S^k_t$,
	\item Awareness consistency: if $s^k_x\in I^k_w$, then $s^k_x=s^k_w$.
\end{enumerate}

Condition 4 is self-explanatory. 
Condition 5 prevents states from being in more than one information set.
Condition 6  says states with different feasible acts cannot be in the same information set (i.e., $i$ can distinguish states based upon differences in their feasible acts).
Condition 7, which is similar to Condition 1 for awareness, ensures that all the states in a given information set are possibilities at a specific time $t$.
The idea is that $i$ is always aware of the time and, therefore, never places any weight on the proposition that he is presently in a state occurring at a time different than the one he is in. 
Condition 8 says that $i$'s awareness of the status of the world's features in states in the same information set must be identical.

Notice the implication that, for each $s_k\in S$, an a awareness state space in a given period, $S^k_t$, is partitioned by its information sets.
Moreover, since the time-stamped state spaces $S^k_t$ themselves partition $S^k$, the information sets also partition $S^k$.
Therefore, let $\mathcal{I}^k$ denote the collection of all information sets associated with $\Gamma^k$.
This allows us to refer to an arbitrary information set simply as $I\in\mathcal{I}^k$.
Then, because Condition 6 says that all states in the same information set have the same feasible actions (of which $i$ is aware), we can write $A^i_I$ without ambiguity.
	
\paragraph{Example: Brian's Infant, Part 3}	
{\em
Information sets are illustrated for the Brian's Infant example in Figure \ref{Diag: p-03}. 
In this scenario, $i$ is in objective state $s_{3}$ and has the same awareness as in Figure \ref{Diag: p-02}. 
Now, information sets $I$ and $I^\prime$ have been added to illustrate the case in which $i$ is uncertain about whether $A$ or $B$ is the best toy.
These are indicated by the dashed lines connecting states. 
$I$ indicates that $i$ knows she is working on $p_1$ but uncertain about which toy is best.
She is aware that she could have been working on $p_2$ had she chosen that problem (according to $I^\prime$).
	
\begin{figure}[h!]
	\centering
	\includegraphics*[page=3,trim = 0in 6in 6in 0in,scale=.8]{Awareness_Diagrams_All}
	\caption{Partially aware infant is also uncertain about which toy is best\label{Diag: p-03}}%trim: L B R TxPer
\end{figure}
}	
When an information set is not a singleton (i.e., $i$ is uncertain about something), we assume she has beliefs about which state is true which are represented by a probability distribution on the states in the information set. 
We use $\rho^i$ to indicate these assessments. 
In the preceding example, $\rho^i(A^\ast,p_1)$ is the probability $i$ assigns to the possibility that the actualized state is $(A^\ast,p_1)$.
	
\subsubsection{Beliefs} \label{sec: beliefs}
Given an objective state $s_k\in S$, let $\Delta(H^k_T)$ denote the set of all probability distributions on the set of subjective terminal histories. 
Then,  $\mu^k\in \Delta(H^k_T)$  is $i$'s belief about how dynamics play out, with $\mu^k(h)$ indicating the probability $i$ places on terminal history $h\in H^k_T$. 

Because act profiles and states imply events in $H^k_T$ by the terminal histories that pass through them, $\mu^k$ can be used to compute conditional probabilities.
In particular, when $i$ is in subjective state $s^k_w\in I\in\mathcal{I}^k$, the probability $i$ assigns to $s^k_w$ is  $\mu^k(s^k_w|I)$.
We adopt the convention that if  $\mu^k(I)=0$, then  $\mu^k(s^k_w|I)=0$.

For example, consider $S^3_1$ in Figure \ref{Diag: p-02} in which each state is in its own, singleton, information set.
Then,  $\mu^{3}((B^\ast,p_1)|(B^\ast,p_1))=1$: $i$ knows she is at awareness state $(B^\ast,p_1)$.
Instead, suppose the situation is identical except that the information sets are as shown in Figure \ref{Diag: p-03}, with $\mu^{3}$ assigning equal probability to both states in $I$.
Then, $\mu^{3}((A^\ast,p_1)|I)=\mu^{3}((B^\ast,p_1)|I)=0.5$.
 
The acts of the individual and Nature at each state imply a true probability distribution on terminal histories for their potential actualization. 
Typically, game theorists are very interested in the relationship between beliefs and this reality.
The array of situations studied run the gamut from perfectly rational (beliefs and reality are perfectly aligned, including with respect to counterfactuals) to subjectively rational (aligned with respect to actualized histories only), to irrational (no alignment at all).
For now, we simply assume that, at any information set, $i$ has beliefs about how the feasible terminal histories will play out (presumably taking into consideration what $i$ expects himself and Nature to do)..



\subsection{Desires} \label{sec: desires}
For all $s_k\in S$, define the state-dependent \textit{desire relation}    $D^k\subset H^k_T\times H^k_T$ where, $(h,h^{\prime})\in D^s$ means that  $i$ in objective state $s_k$ desires the subjective terminal history $h^{\prime}$ at least as much as the history $h$. 
We use the intuitive notation $h\preceq^s h^{\prime}$, which is defined to mean $(h,h^{\prime})\in D^s$. 
We use $\prec^s$ and $\approx^s$ to indicate strict preference and indifference, respectively. 
Assume that each $D^k$ is complete (in the sense that any pairs of histories $h,h^{\prime}\in H^k_T$ are comparable) and transitive. 
Then, the desire relation $D^k$ can be represented by a numerical function $d^k:H^k_T\rightarrow \mathbb{R}$ , where $d^k(h)< d^k(h^\prime)$ and $d^k(h)= d^k(h^\prime)$ if and only if  $h\prec^k h^{\prime}$ and $h\approx^k h^{\prime}$, respectively.

Note the implication that desires may change from objective state to objective state.
The objective state provides a fully-featured snapshot of the world, including the status of $i$'s cognition. 
The desire function says that $i$ is capable of comparing the desirability of complete histories---up to his awareness of the world according to his present, objective state.

We wish to impose some consistency between $i$'s desires over the terminal histories as he is aware of them in a given state and his desires over the objective histories underlying them.\footnote
{
	Remember, the histories of which $i$ is aware at any point correspond to collections of objective histories, the latter being the most refined. 
	Although $i$'s awareness limitations coarsen his perception of reality, the consequences of his actions will correspond to the actual unfolding of a specific objective history. 
	This raises the question of how consistent with reality his desires should be.
}
To this end, let $d:H_T\rightarrow \mathbb{R}$ be the desire function for $i$ under conditions of perfect awareness (i.e., a situation in which $i$ is aware of all the terminal histories and, hence, can assess each one).
Then, we assume that, for all $s_k\in S,h\in H^k_T$, $d^k$ meets the following consistency condition:
\begin{enumerate}
	\setcounter{enumi}{8}
	\item Let 
	\[
		x=\min_{h^\prime\in r^{-1}(h)}d(h^\prime),
	\]
	 and  
	 \[
	 	y=\max_{h^\prime\in r^{-1}(h)}d(h^\prime).
	 \]
	 Then $d^k(h)\equiv\alpha x + (1-\alpha)y$ for some $\alpha\in[0,1]$. 
\end{enumerate}
We imagine that $i$ is equipped with primitive desires ($d$) at the beginning of time which would permit him to compare his desires with respect to any two objective histories, \textit{were he aware of them all} ($H_T$).
When he is not perfectly aware, each history of which he is aware corresponds to an objective diachronic event.
THus, the desire function $d$ implies an interval of values $[x,y]$ for each such event (which may be trivial in some cases---i.e., $x=y$).
Then, the structure of $d^k$ means that $i$'s desires at lower levels of awareness provide an implicit ranking of the objective intervals determined by   $d$.
Notice that the $\alpha$ parameter does not vary by state. 
It is a constant primitive of $i$'s desires from state to state.\footnote
{
	That is $\alpha$ does not vary by $s$.
}
That said, as $i$'s awareness  changes from objective state $s_k$ to objective state $s_l$, the partitions on $H_T$ implied by $H^k_T$ and $H^l_T$ also vary, thereby implying the possibility of variation in the structure of desires from $d^k$ to $d^l$. 

Again, $i$'s desires over objective histories under perfect awareness ($d$) can be thought of as a cognitive primitive that assigns numerical intervals to the subjective histories under imperfect awareness,
Then, in objective state $s_k$, the ranking of subjective histories provided by $d^k$ is consistent with a ranking function on these intervals. 
This is the key consistency condition. 
One interpretation is that this structure ensures that $i$'s desires under unawareness do not depart too far from reality
That is, the individual's valuations of awareness histories are required to be roughly aligned with the evaluations of the objective histories with which they are associated.
This alignment is systematic ($\alpha$ is constant on $S$) and within the bounds of the values that could actually happen ($0\le\alpha\le1$). 
	
Why consider desires over histories? 
Because we assume individuals care about how they get to an end as well as the end itself. 
To take a canonical example, a homeowner may have a renovated kitchen in mind as the desired end. 
However, even if the kitchen specs are provided in extensive detail (so the owner knows exactly what the end will be), there may be many contractors who can deliver it. 
In this case, assuming there are several contractors from which to choose, each of which identify with a different path with states encoding costs  at each step of the way and the final quality of the work, the owner's choice will be based upon the path (costs) as well as the final state (quality). 
Similarly, an individual sensitive to the time value of money will prefer shorter paths to longer ones, other things equal. 
Or, individuals may value portions of the paths themselves.
For example, even though a student drops out of school (thereby, not completing the degree), he or she may nevertheless value the portion of the education that was completed. 
Our approach allows for special cases in which all these details are elaborated as primitives of the situation. For our discussion, we simply assume preferences are over paths.    


	
	
\subsection{Intentions} \label{sec: intentions}
	
For all $s_k\in S$, define the state-dependent \textit{intention} for the individual as $\gamma^k\subseteq H^k_T$, where $\gamma^k=E$ means that in objective state $s_k$ individual $i$ intends subjective event $E$. 
We assume that individuals have desires and beliefs in all states, but not necessarily intentions. 
The idea here is that, e.g., in some states Mike intends the end ``Mike has a cup of coffee'' and in others, Mike has yet to form intentions.
We adopt the convention that $\gamma^k=\emptyset$ means that $s_k$ is a state in which individual $i$ has not formed an intention. 
We highlight that states may be differentiated only by changes in mental attitudes. 
For example, it may be that the only change from $s_k\in S_t$ to $s_l\in S_{t+1}$ is $\gamma^k=\emptyset$ to $\gamma^l=E\ne\emptyset$.
This suggests that the interval between time periods may be very short (measured in milliseconds).
	
This raises the question of how an individual moves from being in a state without an intention to one in which the intention is formed. 
Here, we can require an act of commitment to cement the intention. 
That is, if $s_k\in S_t$ is an objective state in which $i$ does not have an intention, then the set of objective feasible acts, $A^i_{s_k}$, can include an \textit{act to form the intention} (e.g., to ``get a cup of coffee''), which would then take him to a state $s_l\in S_{t+1}$ in which $\gamma^l=E$ where $E$ contains all the states consistent with $i$'s intention (e.g., having a cup of coffee).
	
Summing up, in each objective state, individual $i$'s \textit{cognitional attitudes in $s_k$} are summarized by a profile $\theta^k\equiv(\Gamma^k,\omega^k,\mu^k,D^k,\gamma^k)$.
Then, $i$'s \textit{type} is $\theta\equiv(\theta^0,\ldots,\theta^m)$, itself a profile that describes $i$'s mental attitudes at any potential state in $\Gamma$.
In setting up cognitional  attitudes in this way, we are following a version of the familiar ``type-space'' approach used in game theory \citep[See][]{Harsanyi1967, Mertens1985a}. 
Thus, $i$'s subjective life  is fully described by $\theta$.
	


\section{Notation Reference}
Table \ref{Tab: Notation} elaborates all the mathematical objects used in the paper [TBD].
\begin{table*}\centering
\ra{1.3}
\begin{tabular}{@{}rll@{}}\toprule
Object                                             & \multicolumn{1}{c}{Description}                       & \multicolumn{1}{c}{Comments}\\ \midrule
%$N\equiv \{1,\dots,n\}$                            & The set of $n$ individuals                            & \\
%$i\in N$                                           & An arbitrary individual                               & $i=0$ is Nature\\
%$S$                                         & All objective states of the world       & $S$ contains all possible states\\
%$S_s$                                       & An arbitrary state in $S^i$                           & $s_t$ the state in period $t$,\\
%$S^s$                   & states of which                & \\
%$\mathcal{S}\equiv \{S^\emptyset,S,\ldots,S\}$ & Lattice of awareness spaces                           & Maximum is $S$ and minimum is $S^\emptyset$\\
%												   &													   & $\mathcal{S}_t,S^i_t$ lattice \& state spaces in $t$\\
%$S^i\succeq S^j$                                   & Individual $i$ is at least as aware as $j$            & \\
%$S\equiv\bigcup_{i\in N}S^i$                  & Union of the individual spaces                        & \\
%$r^{i\rightarrow j}(s^i)$                          & Impoverished version of $s^i$ perceived by $j$        & Only defined if  $S^i\succeq S^j$\\
%$B^{\downarrow}$                                   & A synchronic event $B^{\downarrow}\subseteq S$   & $B^{\downarrow}=\bigcup_{S^j\in \mathcal{S}}\left(r^{0\rightarrow j}\right)^{-1}(B)$, $B\subseteq S$\\
%$A^i_s$                                           & $i$'s feasible acts in state $s\in S$               & $a^i_s\in A^i_s$,  arbitrary act by $i$ in $s$\\
%$a_s$                                       & A profile of acts in $s$, $a_s\equiv(a^n_s,a^i_s)$ & \\
%$A_s$                                    & All act profiles in $s$                               & \\
%$A_t$                                     & All act profiles at time $t$                          & $A_t\equiv \cup_{s\in S_t} A(s)$\\
%$A$                                       & All possible act profiles                             & $A\equiv \cup_{s\in S} A(s)$\\
%$\omega(a_t,s_t)$                       & State actualized in $t+1$ from $s_t$ given $a_t$    & e.g., $s_{t+1}=\omega(a_t,s_t)$\\
%$h_s$                                & History at $s\in S$                                    &  \\
%$\mathcal{H}_t$                                    & Set of all subsets of histories at $t$                & \\
\bottomrule
\end{tabular}
\caption{Notation Reference (TBD)}\label{Tab: Notation}
\end{table*}




\section{Four-Phase Model of Action}

With the previous setup in place, we can now elaborate the model discussed in the Introduction.
The phases are: 1) Problem Selection; 2) Deliberation; 3) Planning; and 4) Acting.
Each phase \textit{requires} completion of the preceding phases before it can begin.
As we elaborate the phases, we also add certain consistency conditions which are pertinent to them.

\begin{figure}[h!]
	\centering
	\includegraphics*[page=4,trim = 0in 3in 0in 0in,scale=.65]{Awareness_Diagrams_All}
	\caption{Overview of states and decision phases.\label{Diag: p-04}}%trim: L B R TxPer
\end{figure}

The objective tree $\Gamma$ elaborates all the ways in which the world can evolve.
Consistent with $\Gamma$, $H$ contains all the terminal histories, each of which elaborates a particular evolution from the beginning state, $s_0$, to some terminal state $T$ periods later.
The elements of $A$ describe the feasible act profiles at every state.
The act mapping $\omega$ connects the act profiles available in one state lead to the new states in the following period which they actualize.
See Figure \ref{Diag: p-04} for an overview of the process.



\paragraph{Brian's Infant, Part IV}
{\em
Figure \ref{Diag: p-04} illustrates $\Gamma$ for the Brian's Infant example. 
Here, we introduce some further details.
First, to illustrate the four phases of the decision-act process, we extend the tree through $T=4$.
Next, we add the details of what could happen at each phase.
At $t=0$, $i$ decides whether to solve $p_1$ or $p_2$.
At the same time, Nature fixes the best toy and the best TV show. 
For now, we assume Nature does not act until the penultimate period.

\begin{figure}[h!]
	\centering
	\includegraphics*[page=5,trim = 0in 0in 0in 0in,scale=.65]{Awareness_Diagrams_All}
	\caption{Objective tree spanning four periods; bolded path will be focal.\label{Diag: p-05}}%trim: L B R TxPer
\end{figure}

In $t=1$, $i$ deliberates and commits to an intention. 
The intention is a commitment to influence events such that a particular diachronic event is actualized.
It is a goal. 
Here, ff $i$ chooses $p_1$, then her intentions can be either to get $A$ or to get $B$.
Alternatively, if she chooses $p_2$, then her intentions can be to either to watch $W$ or  $C$.

With an intention in place at the start of $t=2$, $i$ then formulates a plan by which to accomplish the intention. 
Here, we assume the plan under $p_1$ is to approach and grab the intended toy. 
Under  $p_2$, the plan is to pick up the TV remote and click to the intended show.
Typically, state-contingent plans will be more elaborate, but in this simple example, the plan is a single-step.

With the plan in place at the start of $t=3$, $i$ acts according to the plan.
At this point, we assume Nature acts once again to determine the success of $i$'s act with respect to the intention commitment at $t=1$.
For example, failure could involve $i$ dropping the toy or Brian entering the room, sweeping up the toys, and placing them into a toy box out of $i$'s reach.

To avoid clutter, Figure \ref{Diag: p-05} does not illustrate every history in $\Gamma$.
Still, there should be enough detail elaborated in the diagram to infer the entire structure of the objective tree.
The number of states are expanding through time, from a single state at $t=0$ to ninetysix at $t=4$.
Also omitted from each state's feasible actions for $i$ is the option to halt solving a particular act-problem and begin another.
As discussed, a set of available act-problems is present in every state.
Here, $i$ can always choose to drop what she is doing on one problem and start working on another.
With this option included, the number of histories would multiply significantly, so we omit them---but readers should keep in mind this is always an available option.

Finally, let us quantify $d$ in this example as follows:
\begin{enumerate}
	\item $d(h)=100$ if $h$ includes successfully obtaining the best toy;
	\item $d(h)=20$ if $h$ includes successfully obtaining the second best toy;
	\item $d(h)=50$ if $h$ includes successfully choosing the best TV show;
	\item $d(h)=10$ if $h$ includes successfully choosing the best TV show;
	\item $d(h)=-10$ if $h$ includes failing to achieve any of the above.
\end{enumerate}
Since the states in $S_4$ each correspond to a terminal history, we can write, e.g., $d(s_{57})$ without ambiguity.
Thus, $d(s_{57})=100$, $d(s_{58})=-10$,   and so on.
}

\subsection{Problem Selection}

As discussed above, at the beginning of time, in objective state $s_0$, the individual may select from a set of mutually exclusive act-problems. 
The mutual exclusivity may be due to something inherent in the problems themselves (for example, Brian's infant may be faced with a decision about how to play indoors or how to play outdoors---these are inherently mutually exclusive), or they may be mutually exclusive due to the cognitive capacity constraints (for example, Brian's infant can play with a toy while watching TV, but only has the capacity to deliberate which toy is best or which show is best, but not both simultaneously).

For each objective state, $s_k$, the act-problems that are available for deliberation are given by $P_k$.
At $s_0$, choosing an element in $P_0$ is the \textit{only} act available to $i$.
Individual $i$ assesses which problem to solve at $s_0$ based upon desires $D^0$ and beliefs $\mu^0$.
In any future state, the individual may stop what he is doing and select a new act-problem to solve.

For now, we assume that $i$ is aware of all the possible act-problems at $s_0$.
However, we also assume that $i$ has only a vague sense of the dynamics involved with each of these problems.
Specifically, let $\Gamma^0$ be a tree with $|P_0|$, $T$-length branches (one for each problem in $P_0$).\footnote
{
	So, $i$ is comparing the consequences of solving each problem over an equal time horizon.
}
In this state, $i$ consults his desires and chooses a problem that maximizes $d^0$.
\textit{This choice of an act-problem is the output of of the Problem Selection phase}.


\paragraph{Brian's Infant, Part V}
{\em
Figure \ref{Diag: p-06} illustrates  a partial illustration of the situation in $s_0$ (shown are periods $t=0,1$).
The purpose of this diagram is to show how the state projections work.
The objective tree $\Gamma$ is shown on the top half of the diagram which accounts for all the feasible acts by Nature and $i$.
The bottom half illustrates the corresponding parts of $i$'s awareness tree $\Gamma^{0}$
Here, $i$ is aware of both act-problems that are available to her at the beginning of the decision process.
However, at this early stage, she is only vaguely aware of what each of these problems entail.
She has not analyzed either one in deep detail.
Therefore, as she anticipates the unfolding of events, she is only aware of the broad histories consistent with ``solving $p_1$'' and ``solving $p_2$.''

\begin{figure}[h!]
	\centering
	\includegraphics*[page=6,trim = 0in 1in 0in 0in,scale=.6]{Awareness_Diagrams_All}
	\caption{Partial awareness tree for Brian's Infant at the beginning of time, $s_{0}$.\label{Diag: p-06}}%trim: L B R TxPer
\end{figure}

The complete awareness tree is shown in Figure \ref{Diag: p-07}.
What $i$ envisions as she contemplates which problem to tackle is a sequence in which she chooses either to play with the best toy or to watch the best show.
She knows the four phases required to get to an action.
However, she is not thinking about the details of Nature's act at $t=0$, nor is she aware of her feasible acts at each future state.
She does realize that, at the conclusion of her efforts, she may succeed or fail depending upon events outside of her control.
In this graph, the histories in $\Gamma$ corresponding to abandoning the plan in the last phase (e.g., $s_{59}$) are projected into the fail histories in $\Gamma^0$ (e.g., $s^0_8$)---$i$ is not thinking about them.

\begin{figure}[h!]
	\centering
	\includegraphics*[page=7,trim = 0in .5in 5in 0in,scale=.7]{Awareness_Diagrams_All}
	\caption{Complete awareness tree for $i$ in objective state $s_{0}$.\label{Diag: p-07}}%trim: L B R TxPer
\end{figure}

Notice that the four histories of which she is aware each correspond to twenty-four distinct histories in reality.
For example, the terminal synchronic event associated with $s^0_{7}$ is
\[ 
	{r^{0}}^{-1}(s^0_7)=\{s_{57},s_{60},\ldots,s_{101},s_{104}\}.
\]
These correspond to the diachronic event ``the infant chose  to solve $p_1$ and succeeds in her intended outcome.''
In terms of payoff values, this event includes two kinds of histories: i) those that include success in obtaining the best toy $(d(h)=100)$; and ii) those that include success in obtaining the second best toy $(d(h)=20)$.
With respect to Item (ii), note that at $t=1$ the infant is free to choose either toy as her intended plaything, even though she knows which toy is, in fact, best.
Notice also that intending second best \textit{is} a feasible act at that point (even if her desires motivate her to intend otherwise).

In the  $p_1$/success event, the min value is $20$ and max value is $100$, whereas in the corresponding $p_2$ event, the min value is $10$ and the max value is $50$.
Suppose $\alpha=0.5$. 
Then, the terminal history through $s^0_{7}$ in Figure \ref{Diag: p-07} has value $d^{0}(s^0_{7})=60$ and the one through  $s^0_{9}$  has value $d^{0}(s^0_{9})=30$.
The fail histories terminating in $s^0_{8}$ and $s^0_{10}$  both have value of $-10$.


To choose between $p_1$ and $p_2$, $i$'s beliefs must come into play.
Specifically, $i$ must assess the probability of success or failure associated with each problem. 
Suppose that in state $s_0$, $i$  believes the probability of success, conditional upon being at information set $I=\{s^0_1\}$ (the consequence of choosing $p_1$) or at $I=\{s^0_2\}$ (the consequence of choosing $p_2$) is 0.5 for both problems. 
Then, the expected value of choosing $p_1$ is $0.5*60-0.5*10=25$ and the expected value of choosing $p_2$ is $10$.
Therefore, $i$ chooses $p_1$.
}

\subsection{Deliberation}
The Deliberation phase begins in $t=1$ in objective state $s_k=\omega(a_{s_0})\in S_1$.
During the time interval during which the state of the world evolves from  $s_0$ to $s_k$, the individual shifts his awareness to support a deliberation on how to resolve his chosen act-problem. 
Typically, the awareness tree in $t=1$ will refine certain aspects of the original awareness tree in $t=0$ and coarsen others.
In the new awareness tree, $i$ is aware of the history of his own moves.
He considers the terminal histories that are reachable from his present state and, based upon his preferences and beliefs, forms a concrete intention.

Intentions formed in objective state $s_k\in S$ take the form $\gamma^{k}\subset H^k_T$.
\textit{An intention is the output of the Deliberation phase}.
Here, it is worth reminding ourselves that the awareness state in which $i$ finds himself is given by $s^k_w=r^{k}(s_k)$.
We assume the intention must be feasible according to $i$'s awareness; i.e., the awareness histories included in $\gamma^{k}$ must all pass through $s^k_w$.\footnote
{
	In most cases, the intention will be to effect a single terminal awareness history; i.e., $\Gamma^k=\{h\}, h\in H^k_T$. 
	However, this is not required. 
	For example, the intention ``Mike obtains a fresh cup of coffee,'' may be a diachronic event that includes many states (e.g., in which Mike's work being complete may or may not be true).
}
 

\paragraph{Brian's Infant, Part VI}
{\em
Assume that the world evolves according to Figure \ref{Diag: p-05}; i.e., from objective state $s_0$ to $s_{3}$, where $s_{3}$ is actualized by the action profile $a_{s_0}=((B^\ast,W^\ast),p_1)$
This shift in awareness causes $i$ to have a more refined sense of the opportunities and issues associated with $p_1$.
This is illustrated by the evolution from the awareness situation depicted in Figure \ref{Diag: p-06} to the one depicted in Figure \ref{Diag: p-07}. 
In the latter, $i$ is aware that $B$ is the best toy, that $A$ could have been the best toy, and that she could have embarked on a solution to $p_2$ instead of $p_1$. 
She is not thinking about anything having to do with which show to watch.

\begin{figure}[h!]
	\centering
	\includegraphics*[page=8,trim = 0in 0in 1in 0in,scale=.7]{Awareness_Diagrams_All}
	\caption{Awareness tree for $i$ in objective state $s_{3}$.\label{Diag: p-08}}%trim: L B R TxPer
\end{figure}

There are a number of items of which to take notice at this point.
First, referring to Figure \ref{Diag: p-08}, in $\Gamma^{3} $, $i$ is aware of being in state $s^{3}_{1}$.
This state is the projection of the state  $s_{3}$, $s^{3}_{1}=r^{3}(s_{3})$ depicted in Figure \ref{Diag: p-05}.
As such, $i$ is also aware of his own act leading to this state $(a^i_0=p_1)$ as well as Nature's act $B^\ast$.
From this point, the feasible histories are the ones terminating in $s^{3}_{12}$ through $s^{3}_{15}$.
Notice that, the paths leading to these histories from  $s^{3}_{1}$ constitute a rooted tree: $i$ does not anticipate becoming unaware.
The desire is to succeed at obtaining the best toy.
Therefore, the intention going into period $t=2$ is $\gamma_{3}=\{s^{3}_{12}\}$.
The act sequences along the histories from $s^{3}_{2}$ and $s^{3}_{3}$ are not labeled---since these are now off the path of actualization, $i$ is free to name them however he pleases.
}


\subsection{Planning}
Others have suggested that, in addition to helping us think through how to attain a desired end, plans serve the additional function of unencumbering the mind of some portion of its cognitive load. 
We agree and incorporate this aspect of planning explicitly into our analysis.

To set up our plan formalism, let us begin by describing behavior in $\Gamma$ more generally. 
Specifically, suppose we wish to describe the acts that $i$ and $n$ would take at any state in $s_k\in S$ were it to be actualized---that is, a God's-eye, global view of actor behavior. 
In game theory, this is done via what are called ``behavior strategies.''
The use of the word ``strategy'' is slightly misleading since the purpose of these objects is to provide a global description of an actor's behavior in every state, whether intentional or otherwise.
Given such a description, one can ask various questions, such as whether a particular behavior represents a rational set of state-contingent choices (hence the ``strategy'' moniker).
Because $i$'s intentional ``plans'' will more restrictive than behavior strategies, we drop the word ``strategy'' so as to avoid confusion between the two: i.e., an ``actor behavior'' will be a description of that actor's behavior at every potential state.

A \textit{behavior} for $j\in\{i,n\}$ is a function $\sigma^j:S\rightarrow \Delta(A^j)$ that maps from objective states  to probability distributions over the feasible acts of $j$.
Mapping to probability distributions allows us to represent the uncertainty associated with Nature's acts as well as to allow for situations in which the individual randomizes over acts (e.g., $i$ decides to ``flip a coin'' to determine what to do).
We write $\sigma^j_k(a^j)$ to indicate the probability that $j$ chooses act $a^j\in A^j_k$ conditional upon finding himself at state $s_k$.
Intuitively, we require $\sigma^j_k(A^j_k)=1$; i.e., when at state $s_k$, the randomization is over the feasible acts at $s_k$.
If $j$ takes act $a^j_s$ with certainty, then $\sigma^j(a^j_s) =1$. 
If $s^\prime\in I$

A \textit{behavior profile} is a pair $\sigma\equiv(\sigma^i,\sigma^n)$.
Let $\Sigma$ be the \textit{set of all possible behavior profiles}, and $\Sigma_k\equiv \Delta(A^n_{k})\times\Delta(A^i_{k})$, the \textit{set of behavior profiles at $s$} with typical element $\sigma_k=(\sigma^i_k,\sigma^n_k)$.
It is not hard to show that every behavior profile induces a probability distribution on terminal histories, $H_T$.

Now, a  plan is a set of state-contingent instructions intentionally developed by $i$ in a particular awareness state $s^k_w\in S^k$ that is designed to cause $i$ to achieve the intended event $\gamma^{s^k_w}$.
Plans are a restricted form of behavior. 
We use the former to describe an explicit, intentional set of instructions formulated and adopted by the individual and the latter to describe overall behavior in $\Gamma$, including that of the individual and nature.

Suppose $i$ is in objective state $s_k\in S$ and $s^k_w=r^k(s_k)$ is $i$'s corresponding awareness state in $\Gamma^k$. 
Let $\bar{\Gamma}^{k}$ denote the subgraph of $\Gamma^k$ beginning at awareness state  $s^k_w$ (recall, the paths emanating from $s^k_w$ are required to be a tree) with corresponding nodes $\bar{S}^{s}$.
Then, $\bar{\sigma}^{k}$  is the \textit{action plan at $s$}, where for each awareness state $s^k_x\in \bar{S}^{k}$, $\bar{\sigma}^{k}(a^{k.i}_{x})$ indicates the probability with which the feasible action $a^{k.i}_{x}$ (of which $i$ is aware) will be chosen at $s^k_x$ according to this plan (which is formulated at in the awareness state corresponding to the objective state $s_k$).


\paragraph{Brian's Infant, Part VII}
{\em
In Figure \ref{Diag: p-08}, we see the world from $i$'s perspective in objective state $s_{2.6}$, which corresponds to awareness state $s^{\prime\prime\prime}_{2.1}$.
In this state, $i$ knows that the intention is to get to $s^{\prime\prime\prime}_{4.1}$.
To achieve this, he formulates a very simple plan---namely, to take the action $a^i_{s^{\prime\prime\prime}_{3.1}}=$ ``get $B$'' in the next period (upon completing the plan at this stage).

The formal objects involved in this phase include $\bar{\Gamma}^{s_{2.6}}$, the subgraph of $\Gamma^{2.6}$ in which $s^{\prime\prime\prime}_{2.1}$ is the root node of the continuation tree with histories terminating in $s^{\prime\prime\prime}_{4.1}$ and $s^{\prime\prime\prime}_{4.2}$.
The awareness states under consideration are $\bar{S}^{s_{2.6}}=\{s^{\prime\prime\prime}_{2.1},s^{\prime\prime\prime}_{4.1},s^{\prime\prime\prime}_{4.2}\}$.
However, the only state at which a feasible move is available is $s^{\prime\prime\prime}_{2.1}$ (since the other nodes are terminal).
Moreover, the only act available is $a^i_{s^{\prime\prime\prime}_{3.1}}=$ ``get $B$''.
Therefore, the plan is $\bar{\sigma}^{s_{2.6}}(a^i_{s^{\prime\prime\prime}_{3.1}})=1$: choose action ``get $B$'' with probability = 1 upon arrival at $s^{\prime\prime\prime}_{3.1}$.

\begin{figure}[h!]
	\centering
	\includegraphics*[page=9,trim = 0in 0in .5in 0in,scale=.6]{Awareness_Diagrams_All}
	\caption{Awareness tree for $i$ in objective state $s_{2.6}$; plan sub-graph highlighted in green.\label{Diag: p-09}}%trim: L B R TxPer
\end{figure}
}


\subsection{Action}

Following the formulation and adoption of a plan, the individual begins to take actions according to the plan.
By providing a script for state-contingent action, the plan frees $i$ from having to devote cognitional resources to thinking about what actions to take.
Thus, once an action plan is adopted, $i$ is free to begin a new act-problem cycle in parallel with the unfolding of his plan.
This process is straightforward and, with most real-world plans aimed at achieving substantive intentions, will span multiple time periods.

A central remaining issue is plan abandonment: given that a plan is operational, when is it rational for an individual to pause and reassess it? 
If one of the main aims of planning is to free up cognitional resources, it  cannot be the case that individuals constantly reassess their plans since to do so would defeat the aim.
On the other hand, plans followed blindly in the face of unfolding events will surely lead to disaster in many instances.
What is the appropriate balance?
One answer, the one that falls naturally out of our framework, is that the individual reassesses when he becomes aware that the world is not unfolding according to plan.

Consider a plan $\bar{\sigma}^{k}$  that is formulated in objective state $s_k\in S_t$.
Suppose the world evolves to state $s_l\in S_{t^\prime}$, $t^\prime>t$.
Assume $i$'s awareness of $s_l$ when he is in state $s_k$ is $s^k_w=r^s(s_l)\in  \bar{S}^k$ and $i$'s awareness of $s_l$ when he is in  $s_l$ is $s^k_x=r^l(s_l)$.
Then, we assume the plan is \textit{automatically followed}  if $s^k_w=s^k_x$.
That is, as long as the features of the world of which $i$ is aware in period $t^\prime$ conform to his expectations.
When this is true, $i$ has no reason to abandon the plan---everything is going as anticipated.
Note that awareness states cannot be equal if they have different histories or different feasible actions.	

When a plan 

\paragraph{Brian's Infant, Part VIII}
{\em
Figure \ref{Diag: p-10} illustrates the situation in which objective state $s_{36}$ is actualized following objective state $s_{14}$.
Arriving in objective state, $s_{36}$, the features of the world of which $i$ is aware is given by $s^{36}_{3.1}$.
Having adopted the plan, $i$ automatically takes action $a^{36}_7=$ ``get $B$'' provided $s^{36}_{7}=s^{14}_{7}$, where the latter was $i$'s awareness of objective state $s_{36}$ when $i$ was in  preceding objective state $s_{14}$ (as shown in Figure \ref{Diag: p-09}).
This is because---from $i$'s perspective---it was in awareness state $s^{14}_{4}$ that she formulated the plan with respect to what to do in the state anticipated to follow, $s^{14}_{7}$.
Provided that her awareness of her actualized state at $t=3$ is consistent with her expectation in the previous period,  she follows through with the plan.

\begin{figure}[h!]
	\centering
	\includegraphics*[page=10,trim = 0in 0in .5in 0in,scale=.6]{Awareness_Diagrams_All}
	\caption{Awareness tree for $i$ in objective state $s_{36}$: plan adopted.\label{Diag: p-10}}%trim: L B R TxPer
\end{figure}

Upon taking her action, Nature moves to determine the ultimate success or failure with respect to achieving her intended outcome.
Importantly, in whichever terminal state she finds herself in $t=4$, $i$ will have begun work on $p_2$. 
Thus, we could extend the tree $\Gamma$ to include an additional 3 periods over which $i$ would act to select the best TV show.
Because $i$ is able to begin work on $p_2$ in parallel with the unfolding of the plan to resolve $p_1$, one time period is saved.
That is, to solve both problems in strict sequence would take eight periods whereas the cognitive capacity created by the plan means both can be done in seven.
}

\section{Exploration of intentional unawareness/focus}
[SHOW NOISE SIGNALS TOY QUALITY EXAMPLE---PARALYSIS BY ANALYSIS AVOIDED]

\section{Exploration of plans}
[SHOW EXAMPLE OF STRAYING OUT OF PLAN?]










%	\subsection{Consistency conditions\label{sec:consistencies}}
%	
%	Having structured the objects of interest, we now explore various conditions required to impose the regularities between the various mental attitudes and between those attitudes and the external world that are appropriate to a rational human being. 
%	
%	\paragraph{Reality Alignment\label{para: reality alignment}} 
%	Beginning with the latter, our setup allows individuals to believe (place positive probability on) things that are not objectively true. 
%	However, it is difficult to square rationality with someone whose beliefs are completely divorced from reality. 
%	Therefore, we assume beliefs align with reality at least to some extent.
%	\begin{condition}[Grain of Truth]\label{cond:grain of truth}
%		For all $i\in N$, $s_t\in S$, $\mu_i^s(h^\ast_t)>0$.
%	\end{condition}
%	\noindent That is, rational individuals do not rule out the true state of affairs. 
%	This implies that, although an indivual's beliefs about an event may be wildly inaccurate, that belief is not completely irrational: i.e., for all $W\in \mathcal{H}$ such that $\mu_i^s(W)>0$, $h^\ast_t\in W$. 
%	Going in the other direction, for all $h^\ast_t\in H^\ast$, there exists some $W\in \mathcal{H}$ such that $\mu_i^s(W)>0$.
%	This condition is not without controversy as it does rule out situations in which an individual is surprised by being confronted with a state of affairs he or she had previously thought impossible.
%	There are formal approaches to dealing with such situations.
%	For now, however, we sidestep such issues.
%	
%	\paragraph{Learning\label{para: learning}}
%	We can also think of consistencies implied by learning. 
%	Even with the Grain of Truth Condition in place, our setup presently allows a person's beliefs through time to be completely inconsistent in all ways except $\mu_i^s(h^\ast_t)>0$. 
%	For example, suppose $X,Y\in \mathcal{H}$ and $\mu_i^{s_t}(X)=1$ and $\mu_i^{s_{t+1}}(Y)=1$ ($X$ and $Y$ contain all the states $i$ believes are possible in periods $t$ and $t+1$, respectively). 
%	Then, even if $X$ and $Y$ are quite large, there is nothing in the setup preventing $X\cap Y= h^\ast_{t+1}$; i.e., the \textit{only} consistency from period to period is belief in the possiblity of the objectively true history.
%	Such situations seem inconsistent with any reasonable concept of learning. 
%	The following condition is a notion of learning that admits a wide range of learning models.
%	For example, Baysian updating is consistent with this (though, by no means requred).
%	\begin{condition}[Weak Learning]\label{cond: weak learning}
%		Let $X,Y\in \mathcal{H}$. For all $i\in N$, $s_t,s_x\in S, x>t$, if $\mu_i^{s_t}(X)=1$ and $\mu_i^{s_x}(Y)=1$, then $Y\subseteq X$.
%	\end{condition}
%	\noindent Notice that learning is, indeed, weak in the sense that one may never learn anything ($Y=X$ through time).
%	However, we imagine that as individuals experience the world, their grasp of it becomes more refined. 
%	Again, this condition is also not without controversy since it seems to rule out ``conversion'' experiences in which an individual shifts from one worldview to another, apparently inconsistent worldview.
%	Whether or not such experiences are, in fact, inconsistent with Condition \ref{cond: weak learning} we leave for another discussion.
%	
%	\paragraph{Introspection\label{para: introspecton}} 
%	It seems reasonable to assume that an individual knows his or her own mental features (but may be uncertain of those of others). 
%	For example, being certain of one's own beliefs rules out some peculiar mistakes in information processing (e.g., \citet{Geanakoplos1989}, \citet{Samet1990}). 
%	As described above, the probability distribution representing an individual’s beliefs in may vary by state.  
%	Introspection entails that, at any given state, the agent's belief assigns probability 1 to the set of states in which he has the same belief as in that state. Formally, 
%	\begin{condition}[Introspection]\label{cond: introspection}
%		For each agent $i \in N$ and  state $ s \in S$, the agent's belief at $s$, $\mu_i^s$, assigns probability 1 to the set of states in which  $i$ has precisely these beliefs: 
%		$\mu_i^s(\{s^\prime \in S \mid \mu_i^{s^\prime} = \mu_i^s\})=1$. 
%	\end{condition}
%	
%	\paragraph{Ordering of desires \label{para: desire ordering}} 
%	It is also typical to add some structure to desires, namely that they be a partially ordered. 
%	Formally, for all $i\in N$, $\preceq_i$ is a partial order relation on the set of paths, $P$; i.e., the following conditions hold for all paths in $G^n$:
%	\begin{enumerate}
%		\item $\forall  p^\prime\in S, (p^\prime,p^\prime)\in D(p)$: the relation ip reflexive,
%		\item $\forall  p^\prime,p^{\prime\prime}\in p,(p^\prime,p^{\prime\prime})\in D(p)\wedge (p^{\prime\prime},p^\prime)\in D(p)\Rightarrow p^\prime=p^{\prime\prime}$: the relation ip antipymmetric,
%		\item $\forall  p^\prime, p^{\prime\prime}, p^{\prime\prime\prime}\in p, (p^\prime,p^{\prime\prime})\in D(p)\wedge (p^{\prime\prime},p^{\prime\prime\prime})\in D(p)\Rightarrow  (p^{\prime},p^{\prime\prime\prime})\in D(p)$: the relation is transitive.
%	\end{enumerate} 
%	These conditions simply assume that there is a certain degree of consistency in an individual's desires over states. 
%	
%	\paragraph{Intentions\label{para: intentons}} 
%	An intention differs from both beliefs and desires in that this mental attitude implies the individual possessing it has made a commitment to take action toward a desired end. 
%	The desired end is an event, such as ``Mike buys a cup of coffee,'' which may be actualized by a large number of states of the world; e.g., buying at McDonalds, or at Starbucks, or alone, or with friends, or while believing the dark roast is probably sold out. Thus, in state $s$, the object of individual $i$'s intention is an event in $\mathcal{S}$. 
%	It is not enough for an individual to simply intend some outcome. 
%	Rather, we assume that at the time an intention is formed, it is coupled with a concrete plan of action designed to achieve the desired end. 
%	
%	To formalize this, for each individual $i$, define an \textit{action plan} as a function $\sigma_i:S\rightarrow A$ where $\sigma_i(s)=a_i\in A_i(s)$ indicates that when individual $i$ arrives at state $s$ she selects an act $a_i$ from the set of acts $A_i(s)$ available at that state. 
%	Since every state has a single history leading to it, action plans may be history-contingent.
%	Notice that, as defined, the action plan indicates what act the individual will implement at every state. 
%	Of course, we do not expect the individual to have thought through a contingency plan for every state in the state space. 
%	Rather, we impose a means-ends consistency condition on $\sigma_i$ that joins the action plan to the intention.
%	
%	\begin{condition}[Weak Means-Ends Consistency]\label{cond:weak M-E}
%		Suppose individual $i$'s intention is given by $\gamma_i(s)=X\in\mathcal{S}$. 
%		Let $P^s_X\subset P$ denote all the paths in $G^n$ that begin at $s$ and terminate in $X$. 
%		Then $\sigma_i$ is said to be \textit{weak means-ends consistent with $\gamma_i(s)$} if at no state $s^\prime$ along any path in $P^s_X$ does $\sigma_i^{s^\prime}$ force actualization of a state $s^{\prime\prime}$ that is not on any path in $P^s_X$. 
%		By ``force'' we mean that $\sigma_i^{s^\prime}$ indicates an act that actualizes some state outside of $P^s_X$ regardless of the acts of all the other individuals and Nature. 
%	\end{condition}
%	
%	\begin{condition}[Strong Means-Ends Consistency]\label{cond:strong M-E}
%		Suppose individual $i$'s intention is given by $\gamma_i(s)=X\in\mathcal{S}$. 
%		Let $P^s_X\subset P$ denote all the paths in $G^n$ that begin at $s$ and terminate in $X$. 
%		Then $\sigma_i$ is said to be \textit{strong means-ends consistent with $\gamma_i(s)$} if at every state $s^\prime$ along any path in $P^s_X$,  $\sigma_i^{s^\prime}$ forces actualization of a state $s^{\prime\prime}$ that continues along a path in $P^s_X$. 
%		By ``force'' we mean that $\sigma_i^{s^\prime}$ indicates an act that actualizes some state on a path in $P^s_X$ regardless of the acts of all the other individuals and Nature. 
%	\end{condition}
%	
%	In other words, Condition \ref{cond:weak M-E} says that the individual's plan never has him unilaterally driving the world to a state from which the intended event cannot be reached. 
%	When this condition is met, it may nevertheless be the case that the world is driven to such a state. 
%	However, this will need to be the result of the acts of others and/or Nature and nothing to do with the acts of individual $i$. 
%	The strong form, Condition \ref{cond:strong M-E}, says that individual $i$ has a plan of action by which he can gaurantee his intended even regardless of what anyone else does.
%	There is another case which is this: no matter what $i$ does, the intended $X$ will happen. 
%	In this case, I do not think we would properly call $X$ intention. 
%	
%	We also need some rationality conditions that tie the preferences over paths to the action plan. 
%	This is subtle because paths are determined by the entire act profile (i.e., and not just the acts of $i$. 
%	So, how do you tie in preferences. One possiblity is to use $i$'s may have beliefs about what the other agents are going to do (remember all of this would be encoded in the states) and, based upon this, choose an action plan that implements the most preferred path possible given the plans of the others. 
%	This would then tie beliefs, desires, intentions and plans of action together. 
%	
%	
%	[STOP HERE]
%	
\pagebreak
	\bibliography{library}
	
\end{document}

pdflatex: --aux--directory=build
bibtex: build/% -include-directory=build