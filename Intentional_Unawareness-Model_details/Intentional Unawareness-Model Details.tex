\documentclass[
%draft,
11pt,
titlepage,
reqno,
%	oneside,
%	twocolumn
]{article}%Draft option puts "slugs" in the margin for overfull lines

%\usepackage{newlattice}%custom package by Gratzer. Use with amsart See book for details.
%Packages loaded by amsart:
%\usepackage{amsmath}%This loads amsbsy, amsopn, amstext
%\usepackage{amsfonts}
\usepackage{amsthm}%This loads amsgen
\usepackage{amsxtra}
\usepackage{geometry}

%\usepackage{pdfsync}
%\usepackage{upref}
%\usepackage{amsidx}
%\usepackage{stmaryrd} %This adds small left arrows for accents that more closely mirror the \vec command
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{latexsym}
\usepackage{amsmath}

\usepackage{exscale}
\usepackage{amscd} %commutative diagrams
\usepackage{dcolumn} %to get decimal places aligned in tables
\usepackage{array}
\usepackage{tabularx}
%\usepackage{MnSymbol} %dashed arrows and more - see documentation
\usepackage[mathscr]{eucal}
\usepackage[english]{babel}
\usepackage[pdftex]{graphicx}
\usepackage{booktabs}%for nice tables (see discuss at https://people.inf.ethz.ch/markusp/teaching/guides/guide-tables.pdf). For details see https://ctan.org/pkg/booktabs.
% A FANCY TABLE
% \begin{table*}\centering
	% \ra{1.3}
	% \begin{tabular}{@{}rrrrcrrrcrrr@{}}\toprule
		% & \multicolumn{3}{c}{$w = 8$} & \phantom{abc}& \multicolumn{3}{c}{$w = 16$} &
		% \phantom{abc} & \multicolumn{3}{c}{$w = 32$}\\
		% \cmidrule{2-4} \cmidrule{6-8} \cmidrule{10-12}
		%     & $t=0$    & $t=1$    & $t=2$  & & $t=0$    & $t=1$    & $t=2$   & & $t=0$    & $t=1$   & $t=2$\\ \midrule
		% $dir=1$\\
		% $c$ & 0.0790   & 0.1692   & 0.2945 & & 0.3670   & 0.7187   & 3.1815  & & -1.0032  & -1.7104 & -21.7969\\
		% $c$ & -0.8651  & 50.0476  & 5.9384 & & -9.0714  & 297.0923 & 46.2143 & & 4.3590   & 34.5809 & 76.9167\\
		% $dir=0$\\
		% $c$ & 0.0357   & 1.2473   & 0.2119 & & 0.3593   & -0.2755  & 2.1764  & & -1.2998  & -3.8202 & -1.2784\\
		% $c$ & -17.9048 & -37.1111 & 8.8591 & & -30.7381 & -9.5952  & -3.0000 & & -11.1631 & -5.7108 & -15.6728\\
		% \bottomrule
		% \end{tabular}
	% \caption{Caption}
	% \end{table*}
\usepackage{subcaption}%for tables and such
\usepackage{lipsum} %for preventing breaks and such
%\usepackage{pgf,pgfarrows,pgfnodes,pgfshade}
\usepackage{setspace} %Turn ON for editing
%\usepackage{verbatim}
%\usepackage{enumerate}
%\usepackage{xspace}`
%\usepackage{longtable}
%\usepackage{epstopdf}
%\usepackage[authoryear]{natbib}
%\usepackage{lscape}
\usepackage{mathabx}
\usepackage{natbib}
\bibliographystyle{chicago}

%\theoremstyle{plain}
\newtheorem{acknowledgement}{Acknowledgement}
\newtheorem{assumption}{Assumption}
\newtheorem{axiom}{Axiom}
\newtheorem{case}{Case}
\newtheorem{claim}{Claim}
\newtheorem{conclusion}{Conclusion}
\newtheorem{condition}{Condition}
\newtheorem{conjecture}{Conjecture}
\newtheorem{corollary}{Corollary}
\newtheorem{criterion}{Criterion}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{econjecture}{Empirical Conjecture}
\newtheorem{example}{Example}
\newtheorem{exercise}{Exercise}
\newtheorem{lemma}{Lemma}
%\theoremstyle{remark}
\newtheorem{remark}{Remark}
\newtheorem*{notation}{Notation}
\newtheorem{proposition}{Proposition}
\newtheorem{theorem}{Theorem}
\newtheorem*{main}{Main Theorem}
\newtheorem{solution}{Solution}
\newtheorem{summary}{Summary}
%\newenvironment{proof}[1][Proof]{\noindent\textbf{#1.} }{\ \rule{0.5em}{0.5em}}

\newcommand{\BigFig}[1]{\parbox{12pt}{\Huge #1}}%See Gratzer l. 2442 (for matrix)
\newcommand{\BigZero}{\BigFig{0}}

\doublespacing %This is a command from the SetSpace package

\geometry{letterpaper}
\setlength{\oddsidemargin}{0in}
\setlength{\topmargin}{0in}
\setlength{\topskip}{0in}
\setlength{\headsep}{0in}
\setlength{\headheight}{0in}
\setlength{\textwidth}{6.5in}
\setlength{\textheight}{8.75in}

%junk comment for Git

\begin{document}
	
	\title{Intentional Awareness\thanks{}
	}
	\author
	{
		Brian Epstein \\Tufts University, Medford
		\and 
		Michael D.\ Ryall \\University of Toronto 
	}
	\date{\today}
	\maketitle
	
	%\begin{abstract}
	
	%\end{abstract}
	
	%\doublespacing
	\def\baselinestretch{1.5}\small\normalsize
	\newcommand{\ra}[1]{\renewcommand{\arraystretch}{#1}}%for tables
	\newpage
	
	
	
	\section{Introduction}\label{sec: intro}
	This note presents the full mathematical description of the intentional awareness model. 
	It is not meant to be a paper.
	Rather, it is a full elaboration of a model that can be summarized or referred to in a paper.
	Some discussion of how certain mathematical objects are intended to be interpreted is provided (though, these descriptions are not at the same level of detail required for a paper).
	
	\subsection{Overview}
	In what follows, we develop a four-phase model of intentional acts. 
	The essential aim of this formalism is to take seriously the cognitive constraints we face as finite, material beings.
	In particular, we proceed from the uncontroversial claim that, at any given moment, an individual can only  attend to some finite number of conscious concerns. 
	We say that an individual is \textit{aware} of the matters toward which his or her attention is directed.
	Under constrained awareness, intentions take on an important role that is distinct from beliefs and desires.
	
	Our approach refines some existing discussions on this topic by distinguishing between states and acts. 
	A state is a snapshot of the world at a given moment in time that describes the status of all the features that are relevant to the situation at hand. 
	An act is a procedure that unfolds over time. 
	Starting in a state of the world at time $t$, the willful acts of individuals and the brute acts of Nature jointly determine the state of the world at time $t+1$.
	This interaction is elaborated in the following sections.
	
	Acts include both efforts that are inherently invisible to others (i.e., mental activities such as deliberating, judging, and choosing) and those that are observable (e.g., enrolling in a graduate course).
	We refer to the latter as \textit{actions} to distinguish them from the sorts of acts that can only be observed by the acting individual. 
	Thus, actions are a subcategory of act.
	Because states of the world include cognitive attitudes, all forms of act have the power to influence future states of the world.
	
	An individual in our model proceeds from an initial state of the world at time $t$ to a future action according to the following sequence of phases. 
	Each phase is assumed to take \textit{at least} one unit of time. 
	During a phase, the individual and Nature may act, thereby bringing the world to a new state. Individuals recall their experiences from earlier phases in later phases.
	\begin{enumerate}
		\item \textbf{Problem Selection:} 
		Contemplating their awareness, beliefs, knowledge, and preferences as featured in the state at $t$, individuals identify the set of act-problems.
		An \textit{act-problem} is an opportunity for the decision maker to achieve a desired goal by influencing the evolution of future states through her acts. 
		The problem is to settle upon a plan by which to cause or contribute to the evolution of the world to a state in which the target goal is attained.
		\textit{	The output of this phase is the selection of an act-problem to solve}.
		Alternatively, the decision maker may choose to wait and evolve to a new state in which they may select another problem for consideration.
		
		The world evolves to a new state.
		\item \textbf{Deliberation:} 
		Contingent upon the act of orientation to engage in an act of deliberation and given the awareness, beliefs, knowledge, and preferences featured in this new state, individuals conduct an analysis to determine which goal should be pursued. 
		Individuals screen out infeasible and dominated goals and then rank-order the remaining ones according to their preferences. 
		\textit{The completion of this analysis is a conclusion about which goal to pursue}. 
		If no goal is best, revert to a new Problem Selection phase.
		
		The world evolves to a new state.
		
		\item \textbf{Judgment:} 
		Contingent upon the goal selected as best and given the awareness, beliefs, knowledge, and preferences featured in this new state, individuals decide whether to pursue the goal. 
		\textit{The output of this phase is a commitment to formulate a plan to achieve the goal}. 
		Failing  that, the individual reverts to a new Problem Selection phase.
		
		The world evolves to a new state.
		
		\item \textbf{Planning:} 
		Contingent upon the commitment to plan and given the awareness, beliefs, knowledge, and preferences featured in this new state, individuals  formulate a state-contingent plan of action. 
		This plan includes the goal in the support of their beliefs (i.e., individuals believe that if the plan is implemented the goal will occur with positive probability). 
		\textit{The output of this phase is a plan and a commitment to activate the plan}.
		Alternatively, the individual may revert to a  new Problem Selection phase.
		
		The world evolves to a new state.
		
		\item \textbf{Acting:} 
		Upon entering the new state, the individual checks her  awareness, beliefs, knowledge, and preferences, then: i) if the state is a contingency included in the plan, then take the action as proscribed; or ii) if not, revert to a new orientation phase.
		\textit{The output of this phase is an action}.
		Alternatively, the individual may revert to a  new Problem Selection phase.
		
		The world evolves to a new state.
	\end{enumerate}
	
	For comparison, Holdon's (p.\ 57) four phase characterization of a typical exercise of freedom of the will unfolds as follows: 
	\begin{enumerate}
		\item \textbf{Deliberating}: Considering the options that are available, and their likely consequences; getting clear on one’s own desires, and one’s own prior plans and intentions; seeing how the options ﬁt in with these desires and plans; establishing pros and cons. 
		\item \textbf{Judging} (deciding that): Making a judgment that a certain action is best, given the considerations raised in the process of deliberation. 
		The upshot of the judgment is a belief. 
		\item \textbf{Choosing} (deciding to): Deciding to do the action that one judged was best. 
		The upshot of this decision is an intention. 
		\item \textbf{Acting}: Acting on the intention that has been made, which involves both doing that thing, and coordinating other actions and intentions around it.
	\end{enumerate}
	
	\paragraph{Comments} The key differences between the two approaches are the following. 
	First, there is a distinction between states of the world and acts which flow over time. 
	Thus, we make explicit the idea that the world is changing as the decision maker tics through the phases. 
	Our first phase recognizes that an individual lands in a state and, at that point, must make some sense of the situation, exercising a certain degree of discretion in  organizing themselves for a deliberation. Holdon does not include such a phase. 
	Our second phase, Deliberation, follows Holdon fairly closely. 
	The main difference is that, in our case, the options are rank-ordered at the end of the phase but with no decision yet to advance to the planning stage. 
	In our Judgment phase, the decision is whether or not to move on to the planning phase (which may be influenced by the evolution of states). 
	Our Planning phase is like Holdon's Choosing phase in the sense that it constitutes the commitment to act. 
	This is because, barring winding up in a state that falls outside the scope of the plan, the individual acts according to plan (there is no reason not to). 
	The difference is that, in addition to the trivial case of simply choosing to do one action no matter what (as in Holdon's case), our plan can be dynamic and state-contingent. 
	Our Acting phases are pretty much identical. The one difference we have in mind is that all the phases are mutually exclusive in our setup except acting. 
	That is, a person can be doing Phase 5 from a previous decision sequence while engaging in a new decision sequence.  
	
	I suggest we combine Phases 2 and 3. Separating out the Judgment phase seems important to Holden. 
	But, I don't see what this adds and rolling in to Phase 2 puts us back to a 4-phase process. 
	Note also that awareness, beliefs, and knowledge are evolving in every one of our phases. 
	This seems to be in contradiction to Holden, where ``beliefs'' arise as a result of a judgment. 
	(I am not even sure how to interpret this, by the way.) 
	On the other hand, it is not clear where, exactly, in our approach, the ``intention'' appears. 
	Each move to a new phase involves a conscious decision to do so. 
	Hence, one could say, there are intentions at every step.
	
	The idea is as follows.
	To the extent some share of the mind's resources are occupied in solving a problem (e.g., deciding what kind of car to buy), those resources are not available for other conscious operations, such as solving other problems, constructing a feasible plan by which to acquire a car, or actualizing that plan by driving to the car dealer and making the transaction.
	We conjecture that an individual's finite stock of cognitive resources almost always acts as a hard constraint on his or her decision- and act-making capability.
	In our model, intentions serve as the pivot from goal choice assessment to goal acquisition planning and implementation.
	The formation of an intention moves an individual from a state of reckoning about what goal to pursue to one in which that choice becomes a commitment accompanied by \textit{plan} by which to attain it.
	Thus, forming an intention frees up the mental resources required to determine which goal to pursue and how to pursue it.
	When events arise consistent with the plan, the individual can proceed accordingly -- without engaging the mental machinery required to reassess goals and plans.
	Because deciding to focus attention on some new problem can, itself, be an intentional goal, one's awareness is dynamic and, to some extent, influenced by one's own intentions.
	As we will see, there are also social implications as individuals become aware of the intentions of others.  
	
	Beliefs and desires will operate in a familiar way. 
	The distinction here is that they are restricted to those matters about which an individual is aware.
	As we show below, because beliefs cannot account for awareness and because intentions shift awareness, a belief-desire model cannot do the work of an awareness-belief-desire-intention model.
	
	\subsection{Awareness}
	
	There are two conditions that must be met for an individual to be aware of some feature of the world. 
	\begin{enumerate}
		\item The feature must be accessible to the individual for active consideration. The sources of accessible features include contemporaneous sense data, imagination, and knowledge---essentially, anything toward which an individual can mindfully attend.
		\item An individual must choose to incorporate that feature into a conscious thought process.
	\end{enumerate}
	These conditions reflect our focus on decision making, as opposed to what mental phenomena might be going on when an agent sits idly thinking with no purpose in mind. 
	Because conscious capacity is finite, at any given moment an individual will be aware of a small subset of all the features constituting the state of the world.
	
	In some cases, a feature of the world may force itself into an individual's awareness through sense data, such as the pilot becoming aware of an alarm screeching in the cockpit.
	Even though the pilot does not control the breaking-through of the noise into his consciousness when the alarm goes off, at the point it does he then has a choice as to whether to incorporate the fact of the alarm sounding into his decision process or not.
	If for some reason the pilot should decide that the alarm is not relevant to any of his active decision deliberations, we count him as being unaware of it---even though it remains audible (and even irritating to listen to), it is not a factor in any deliberation he is presently undertaking.
	Alternatively, a feature of the world may be intentionally called to mind, such as when a pilot in mid-flight calls upon his knowledge of how to navigate the jetliner. 
	One cannot bring to mind aspects of the world that one does not know or cannot imagine, such as an airline pilot who has never been to medical school pondering the technical pros and cons of cutting-edge heart transplant procedures. 
	
	Central to our approach is the assumption that humans face extreme constraints in the number of features of the world to which they can mindfully attend in any given moment.
	Given these constraints and the fact that humans are constantly flooded with more information than they can effectively incorporate into any deliberation, we see that mental effort is required to keep relevant information in mind, as opposed to being required to banish unnecessary information from it.
	For example, unless the pilot chooses to maintain awareness of the cockpit alarm (presumably, because it is relevant to something he is trying to do), we are claiming that the fact of the alarm will automatically fade to unawareness as part of a natural process of the pilot's  cognitive architecture. 
	
	Although this strikes us as an uncontroversial position, it has non-trivial implications.
	In particular, an open issue in the philosophy of action is whether it is rational for individuals to make commitments to ignore new information which, properly considered, might cause them to change their future plans.
	From our perspective, ``ignoring'' information---in the sense of being unaware of it---is the baseline state of most information accessible for human cognition.
	Given that an individual has the mental capacity to focus upon only a tiny fraction of the world's features at any one time, the question for rationality is not whether one should reflect upon the set of all relevant information and then decide whether to brush some of it aside.
	Rather, it is to determine which one of the multitude of issues that are rendered mutually exclusive for the purpose of reflection (due to cognitional constraints) should be brought to into active consideration (at the expense of the benefits available from reflecting upon some other things instead).
	
	Awareness and unawareness have long been a tricky problem for decision theorists. 
	%A decision maker can only choose among the potential acts about which she is aware. 
	%Moreover, in addition to uncertainty about the future consequences of an act, the decision problem may be confounded by unawareness of information relevant to the situation at hand. 
	%The problem is how to model what happens in a dynamic setting when the decision maker suddenly faces an unexpected consequence. 
	For example, in a standard Bayesian decision problem, unawareness of certain consequences could be modeled as zero-probability states according to the decision maker's subjective beliefs. 
	However, Bayesian decision makers will be confounded should a subjectively impossible state occur. 
	What then?
	Added to this is the problem of representing interactive decision makers with different states of awareness (i.e., when the acts of one affects the consequences of the acts of the others). 
	
	\citet{Dekel1998} demonstrate that standard state-space approaches cannot model unawareness. 
	\citet{Schipper2015} surveys various alternatives to modeling unawareness, including approaches from  AI, logic, and game theory. 
	We adopt a version of the framework developed in \citet{Heifetz2006} \citep[also see][for related extensions]{Heifetz2008,Heifetz2013} that is both simpler, in the sense that we focus upon a single-agent decision problem, and extended, in the sense that an agent's space of awareness may vary.
	
	
\section{Notational conventions}

Capital letters ($G$, $N$, etc.) refer to sets and to set-valued correspondences.  
Small Arabic and Greek letters refer variously to elements of sets (e.g., $i\in N$) and functions (e.g., $\sigma:N\rightarrow \mathcal{N}$). 
Terms  are \textit{italicized} at the point of definition.  
A \textit{profile} is a placeholder for a list of elements.
We denote these in boldface: e.g., $\mathbf{x}$ where $\mathbf{x}\equiv(x_1,\ldots,x_n)$. 
The ``$\equiv$'' symbol indicates the definition of a mathematical object. 
If $X$ is a set, then $2^X$ denotes the set of all subsets of $X$. Calligraphic letters refer to sets of sets (e.g. $\mathcal{X}\equiv 2^X$). 
Curly parentheses indicate sets, typically in defining them (e.g. $X\equiv\{x|x\text{ is an even integer}\}$). 
The notation ``$|\cdot|$'' indicates set cardinality (e.g., if $X\equiv\{a,b,c\}$, then $|X|=3$). 
If $X$ is a set and $Y\subset X$, then $X\setminus Y$ is the set $X$ minus $Y$; i.e., the set of elements of $X$ that  remain when the elements of $Y$ are removed. 
All sets are assumed to be finite unless otherwise indicated.\footnote
{
	In almost all cases, our results extend to uncountably infinite sets (e.g., the domains and ranges of continuous variables). However, extending the analysis to include these would involve bulking up the discussion with technical material that would add little, if anything, to the conceptual content of the model.
}
	
	
\section{Objective Reality ($\oplus$)}\label{sec:world}
	
In this section, we describe the status and dynamics of reality---the world as it actually is, could have been, or might yet be along with the causes that drive it to unfold in a particular way. 
Once we describe the way the world works here, we move on to the next section to describe the way a single decision maker understands and thinks about it.
In this setup, there are two \textit{actors}, a human decision maker and Nature, labeled $i$ and $n$, respectively.\footnote
{
	In future work, we will investigate interactions between agents and group decisions.
		This model lays the foundation for those extensions.
} 
Nature is included to account for  a God's-eye view of the status of the world in all its richness as well as for the phenomena that occur outside the decision maker's acts that, jointly with them, determine the instantiations of the world through time.
We focus on the action over a fixed period, from $t=0$ to $t=T$. 
Time is indicated with subscripts.
	
We attach ``$\oplus$'' superscripts mathematical objects to indicate they correspond to objective reality.
The ``$\ominus$'' superscript is used to indicate an object corresponds to the perceptions of the individual.
In general, $\oplus$-objects are richer and more refined than $\ominus$-objects. 
We use $n$ and $i$ superscripts to indicate that an object belongs to or is chosen by Nature ($n$) or the individual ($i$).
	
	
	
	
\subsection{States of the world\label{sec:states}}
	
As outlined in the Introduction, at time $t$, individual $i$ finds himself in a particular situation, which we term an \textit{objective state of the world}.
This state corresponds to objective reality, including the status \textit{of all features of the world} in that moment. 
Importantly, these include both the  mind-independent features of the world as well as the \textit{mental attitude} of the individual acting in that world. 
Let $S^\oplus$ denote the \textit{objective state space}.
$S^\oplus$ is a (finite) set of all the objective states  that can possibly be instantiated from $t=0$ to $t=T$, with typical element $s\in S^\oplus$.
The elements of $S^\oplus$ allow us to consider counterfactuals and future possibilities---that is, it permits description of the way the world was and is, the way it might have been, and the way it could be.

Let $S^\oplus_t\subset S^\oplus$ denote the collection of states that constitute the \textit{objective state space at time $t$}. 
If $t$ is the present or some historic time, then some  $s\in S^\oplus_t$ is factual and the others are counterfactual. 
If $t$ is some future time, then $S_t$ elaborates all the possibilities that could occur at $t$.

In terms of interpretation, it does no harm to imagine that a state, $s\in S^\oplus$, elaborates an uncountably infinite number of features of the world.
However,  we have assumed that the the number of states in $S^\oplus$ is finite.
The rationale for this is two-fold.
First, the number of features of the world that are relevant to an individual decision maker in a specific state is often finite (though, possibly quite large).
Moreover, the number of possible instantiations of each feature may be finite or effectively approximated by a finite number of  categories (e.g., profits in dollars or temperature ranges).
If so, then the number of states required to elaborate all the possibilities is also finite.
Second, even  though we lose a measure of generality by making this assumption, doing so eliminates a substantial amount of mathematical complexity that, were it included to account for an uncountably infinite number of states, would add little in the way of philosophical insight.

%Therefore, assume that there is a finite number $m$ of features of the world relevant to the analysis at hand. Assume that the (finite) set of instantiations of feature $1\le k\le m$ is given by $F_k$ with typical element $f_k$. 
%Then, $F\subseteq F_1\times\cdots\times F_m$ is a set of profiles (possibly correlated) listing \textit{all the potential configurations of features of the world} from $t=1$ to $t=T$.
%Given a state $s\in S^\oplus$, $f_s\in F$ denotes the status of all features of the world in state $s$. 
	
	
	
\subsection{Acts}\label{sec:acts}
	
In our approach the acts of Nature and the decision maker jointly cause the system to evolve from a state $s\in S^\oplus_t$ to a new state $s^\prime\in S^\oplus_{t+1}$.
Acts of Nature represent all the causes that, in conjunction with the act of the individual, co-determine the actualization of a particular state from an immediately preceding, previously actualized state.
	
For each  $s\in S^\oplus$, let $A^j_s$ indicate the set of \textit{feasible acts available to actor $j$ in state $s$} with arbitrary element $a^j_s\in A^j_s$, where $j\in\{i,n\}$.\footnote
{
	Because we consider the intentional formation of some mental attitudes as choices available to individuals, we use the term ``act'' to describe the choices available to someone in a broad way.
	We think of ``action'' as describing the narrower category of act associated with physical movement.
} 
We adopt the convention that $A^j_s=\emptyset$ indicates that actor $j$ has no feasible acts in state $s$.
An \textit{objective act profile at $s$}  is a pair of acts, $a_s\equiv(a^i_s,a^n_s)$, one by Nature and one by the individual, respectively. 
The set of \textit{all objective act profiles at state $s$} is $A_s\equiv A^i_s\times A^n_s$.
Note the implication that the acts of $i$ and $n$ at $s$ are simultaneous---that is, while $i$ is in the process of acting, there are other things going on in the world that may also have an impact on the features of the world of interest to $i$.
Also, keep in mind that the act profiles in $A_s$ are unique.\footnote
{
	To reduce clutter, we do not include $\oplus$ and $\ominus$ superscripts when the association is clear from the context. Thus, $A_s$ when $s\in S^\oplus$ is the objective set of act profiles---implying that adding a superscript, as in ``$A^\oplus_s$,'' is unnecessary.
} 
The set of \textit{all possible objective act profiles at time $t$} is  $A^\oplus_t\equiv \bigcup_{s\in S^\oplus_t} A_s$; and the set of \textit{all possible objective act profiles} is $A^\oplus\equiv \bigcup_{s\in S^\oplus} A_s$.
 
 We wish to represent the uncertainty associated with Nature's acts as well as to allow for situations in which the individual randomizes over acts (e.g., $i$ decides to ``flip a coin'' to determine what to do).
 Then, for each objective state $s\in S^\oplus$, a \textit{mixed state-contingent act for $j\in\{n,i\}$} is a probability distribution over feasible acts at $s$, denoted $\sigma^j_s\in \Delta(A^j_{s})$.\footnote
 {
 	Where $\Delta(A^j_{s})$ is the set of probability distributions over actor $j$'s feasible acts at $s$. 
 } 
We write $\sigma^j_s(a^j_s)$ to indicate the probability that act $a^j_{s}$ is selected at $s$.
The use of probability distributions over feasible acts provides a nice level of generality.
Typically, individuals make action choices with certainty: e.g., $\sigma^i_s(a^i_s) =1$ for some $a^i_s\in A^i_s$.
Thus, mixed acts are general in the sense that they can represent both deterministic and random behavior.
Mirroring the objective act objects, define:
\begin{enumerate}
	\item $\Sigma_s\equiv \Delta(A^n_{s})\times\Delta(A^i_{s})$, the \textit{set of mixed act profiles at $s$} (where the membership of $s$ in an objective or subjective state space will be clear from the context), with typical element $\sigma_s=(\sigma^i_s,\sigma^n_s)$;
	\item $\Sigma^\oplus_t\equiv \bigcup_{s\in S^\oplus_t} \Sigma_s$, the set of \textit{all possible mixed act profiles at $t$}, with typical element $\sigma_t$; and
	\item $\Sigma^\oplus\equiv \bigcup_{s\in S^\oplus} \Sigma_s$, the set of \textit{all possible mixed act profiles}, with typical element $\sigma$.
\end{enumerate}

 
 
 
	
\subsection{Dynamics} 
	
As indicated above, the act profiles summarize all the conditions required to actualize one state from the previously actualized state. 	It may be helpful to think of act profiles as the ``flow'' variables between states---the activities that occur over a unit of time that cause the world to move from one state to the next.
	
To formalize this,  let $\Gamma^\oplus\equiv( S^\oplus,\precdot)$ be the elaboration of all objectively possible sequences of states.
Specifically, assume $\Gamma^\oplus$ is a directed, rooted tree with nodes $S^\oplus$ ordered by the precedence relation $\precdot$; i.e., the predecessors of each $s\in S^\oplus$ are totally ordered by $\precdot$. 
Thus, $S^\oplus_0=\{s_0\}$, where $s_0$ is the root node at the beginning of time.
Let $\omega:A^\oplus\rightarrow S^\oplus$ be a function mapping act profiles available at a given state to its children.	
Thus, if $\omega(a_s)=s^\prime$, it means that when the act profile $a_s$ is taken at $s$, it actualizes the immediately following state $s^\prime$; e.g., if $s\in S_t$, then $\omega(a_s)\in S_{t+1}$.
Assume $\omega$ is bijective from $A_s$ to the immediate successors of $s$ (the elements in $A_s$ uniquely label the edges from $s$ to its successors, one-to-one with no extras or shortfalls).
This means there is no state in which a specific act profile can lead from one state to more than one successor state.\footnote
{ 
	Two or more states may yet have the same the set of feasible act profiles. That is, for $s\ne s^\prime$ it it is possible that $A_s=A_{s^\prime}$. For example, at 11am $i$ may be finished with  work and get a cup of coffee or, alternatively, may not be finished with work and, yet, still have the option to get a cup of coffee. However, it is never the case---holding Nature's acts constant---that getting a cup of coffee in one state has an ambiguous effect on the future.
}
Because the relationship is bijective, we can also consider $\omega^{-1}(s^\prime)=a_s$.

Assume that at the beginning of time, in state $s_0$, $i$ must choose one from  a number of mutually exclusive act-problems.
In future periods, $i$ is generally free to continue working on the current problem or abandon it and take up another. 
In $s_0$, however, $i$'s only feasible act is to choose a problem to get started upon. 
Therefore, $A^i_{s_0}=\{p_1,\ldots,p_k\}$ is the indexed set of $k$ act-problems objectively available to $i$ in state $s_0$.
To refer to \textit{the act-problems available in an arbitrary state, $s$},we define $P_s\equiv \{p_1,\ldots,p_k\}$.\footnote
{
	In later periods, $t>0$, $A^i_s$ may contain numerous acts in addition to choosing a problem from $P_s$. Hence, the distinction is helpful.
}
	
For each state $s$, let $h_s=(s_0,\ldots,s)$ denote the \textit{history at} $s$; i.e., the unique path from $s_0$ to $s$ in $\Gamma^\oplus$.
Because $\omega$ is bijective on $A_s$, which contains no duplicates, each history $h_s$ is also associated with the unique sequence of act profiles that actualize it.
Abusing notation a bit, let this be represented by  $\omega^{-1}(h_s)\equiv(\omega^{-1}(s_0),\ldots,\omega^{-1}(s))$.

Let $H^\oplus$ denote the set of objective \textit{terminal histories}; i.e., of the form $h_s=(s_0,\ldots,s)$ where $s\in S_T$.
It is not difficult to show that each $\sigma\in\Sigma^\oplus$ implies a probability distribution on the set of terminal histories.
We will abuse notation some more and write $\sigma(h)$ to be the probability of terminal history $h$ implied by $\sigma$.
		
\paragraph{Example: Brian's Infant, objective reality}	
We consider the problem of Brian's Infant, an extended example that we will use  to illustrate the formalism as we develop it.
The situation is as follows.
Brian's child, $i$, finds herself in $t=0$ presented with two, mutually potential act-problems: to choose a toy with which to play or to choose a TV show to watch. 
Let the toys be labeled $A$ and $B$. 
One of these toys is better than the other. 
Let $A^\ast$ indicate that $A$ is best and $B^\ast$ indicate that $B$ is best. 
Alternatively, the child can choose to watch TV show $W$ or $C$.
One of these shows is better than the other. 
Let $W^\ast$ indicate that $W$ is best and $C^\ast$ indicate that $C$ is best.


\begin{figure}[h!]
	\centering
	\includegraphics*[page=1,trim = 0in 1in 0in 0in,scale=.6]{Awareness_Diagrams_All}
	\caption{Evolution of Nature's state spaces from $t=0$ to $t=1$\label{Diag: p-01}}%trim: L B R TxPer
\end{figure}

Let $P_{s_0}=\{p_1,p_2\}$, where $p_1$ is the choose-a-toy problem and $p_2$ is the choose-a-show problem.
The idea is that each act-problem presents an actor with a distinct choice deliberation that occupies their full awareness capacity.
That is, cognitive constraints render the elements of $P_{s_0}$ mutually exclusive.
Here, $i$'s feasible acts in state $s_0$ are  $A^i_{s_0}=\{p_1,p_2\}$.
At the same time, Nature determines which toy and which show are best.
Therefore, Nature selects one of among four possible acts: $A^n_{s_0}=\{(A^\ast,W^\ast),(A^\ast,C^\ast),(B^\ast,W^\ast)$, $(B^\ast,C^\ast)\}$.\footnote
{
	When $a^j_s$ is a compound act, as is the case with Nature in this example, we use parentheses to list the individual elements of the act.
}

Figure \ref{Diag: p-01} illustrates the evolution of the system from $t=0$ to $t=1$. 
From $s_0$, the objective action profiles are of the form $a_{s_0}=((Y,X),Z)$ where $a^n_{s_0}=(Y,X)$ is Nature's choice of  $Y$, the best toy, and $X$, the best show, and $a^i_{s_0}=Z$ is $i$'s choice of which act-problem to solve.
Then, $S_1$ contains eight possible states (one for each combination of these variables, as shown).

Here we illustrate the notational convention of identifying states by time and index number by using ``$t.\#$'' subscripts.
For example, $s_{1.3}=(B^\ast,W^\ast,p_1)$ is state number 3 in period 1.
In this state: $B$ is the best toy; $W$ is the best show; and $i$ has decided to focus on choosing the toy with which to play.
This state was actualized by the act profile $a_0=((B^\ast,W^\ast),p_1)$.
The mapping function with respect to this state is $\omega((B^\ast,W^\ast),p_1)=s_{1.3}$.
The history associated with this state is $h_{s_{1.3}}=(s_0,s_{1.3})$.
This history corresponds to an act profile  sequence that includes a single element:  $\omega^{-1}(h_{s_{1.3}})=(a_0)$ where $a_0=((B^\ast,W^\ast),p_1)$.
	
	
\subsection{Events}

The term `event' is used differently in philosophy than it is in probability theory. 
Since we are writing to audiences familiar with one or the other, it is important to clarify this difference. 
In probability theory, `event' is used similarly to the term `property' in philosophy, where properties are understood intensionally. 
Philosophers typically use `event' to mean a spatiotemporal particular extended over time. 
We refer to events associated with states at a moment in time (the probability theory usage) as \textit{synchronic events}, and those associated with properties extended through time (closer to the philosophy usage) as \textit{diachronic events}.

We define synchronic events as subsets of a state space at a moment in time. 
For example, the event ``Mike intends to get a cup of coffee at $t$,'' includes \textit{all} the states in $S_t$ in which getting a cup of coffee is the intention of Mike. 
In philosophical terminology, this is equivalent to the property \textit{being in a state in which Mike intends to get a cup of coffee}, where the intension of the property is all the states of the world in which the world exemplifies that property.

In the Brian's child example in Figure \ref{Diag: p-01}, the synchronic event ``$A$ is the best toy,'' is the subset $\{s_{1.1},s_{1.2},s_{1.5},s_{1.6}\}\subset S_1$.
Alternatively, the event, ``$i$ is solving the play-indoors problem," is $\{s_{1.1},s_{1.2},s_{1.3},s_{1.4}\}$.
Thus, if $i$ is aware of all the states in $S_1$ and knows that ``$A$ is the best toy,'' and that she is solving the play-indoors problem, then she knows that one of $\{s_{1.1},s_{1.2}\}$ is the actual state of the world.

Diachronic events are defined as subsets of the set of terminal histories, $H^\oplus$.
These refer to sequences of events over time and, correspondingly, the sequences of action profiles that cause them.
For example, the set of terminal histories in the Brian's child example is
\[
	H^\oplus=\{(s_0,S_{1.1}),\ldots,(s_0,S_{1.8})\}.
\]
The diachronic event, ``$i$'s period 0 act is $a^i_0=p_1$,'' is given by the subset
\[
	\{(s_0,S_{1.1}),(s_0,S_{1.2}),(s_0,S_{1.3}),(s_0,S_{1.4})\}\subset H^\oplus.
\] 
Diachronic events that will be of interest are individual acts, sequences of act profiles, and features of the world that persist through time---all of which are associated with sequences of states which, themselves, are associated with subsets of $H^\oplus$.
For example, the diachronic event \textit{state $s$ occurred} is the set of terminal histories that include state $s$ as a component.
These are 

It is worth noting that synchronic events imply diachronic events.
For example, the synchronic event, ``$A$ is the best toy in $t=1$,'' corresponds to the diachronic event that includes all histories in which this is true.\footnote
{
	A synchronic event is a subset of states at some time $t$. Its diachronic counterpart is the subset of all terminal histories that pass through those states.
}


	
\section{Awareness}
Our basic approach is to assume that, in each objective state of the world, the individual has in mind a subjective version of $\Gamma^\oplus$. 
Specifically, for each $s\in S^\oplus$, define \textit{$i$'s awareness tree at $s$}, $\Gamma^\ominus_s\equiv(S^\ominus_s,\precdot_s)$, which is also a rooted, directed tree that represents $i$'s subjective awareness of $\Gamma^\oplus$ when he is in $s$.
The elements of subjective state space $S^\ominus_s$ describe the features of the world of which $i$ is aware in objective state $s$.
These include $i$'s awareness of the world's features in past actualized states, the present state, counterfactuals to these, and future states.
Thus, for $s\ne s^\prime,S^\ominus_s\cap S^\ominus_{s^\prime}=\emptyset$ because even if $i$ is aware of the same features of the world in both $s$ and $s^\prime$, the fact that these objective states are themselves distinct implies that the corresponding subjective states are also distinct.
For example, $i$ may be completely absorbed by the same cat meme on his computer even though in one state the day is overcast and in the other it is raining. 
We count these as distinct subjective states.

Let $S^\ominus\equiv \bigcup_{s\in S^\oplus}S^\ominus_s$ be the set of all subjective states in which $i$ could potentially find himself.
Because the subjective state spaces are distinct from one another, the spaces $S^\ominus_s$ form a partition of $S^\ominus$.
The set $S^\ominus_t\subset S^\ominus$ is the collection of \textit{all} the subjective states that could occur in period $t$, depending upon which actual state arises.
When we know the objective state is $s$, then $S^\ominus_{t,s}\subset S^\ominus_s$ is \textit{the set of period $t$ states of which $i$ is aware according to $\Gamma^\ominus_s$ when $i$ is in $s$}.

Most of the notation for objective objects transfers to their subjective counterparts without ambiguity provided we identify which states are objective and which subjective (or understand these from the context).
For example, $H^\ominus_s$ is the set of full-length histories according to $\Gamma^\ominus_s$.
In some cases, double state references are required, one for a subjective state and one for the objective state with which it is associated.
Thus, given $s\in S^\oplus$ and $s^\prime\in S^\ominus_s$, $A_{s^\prime,s}$ indicates the feasible act profiles available at subjective state $s^\prime$ according to $i$'s awareness as specified by the objective state $s$.
Similarly, we define the \textit{subjective actualization function} for $s\in S^\oplus$, $\omega_s$, such that, for each $s^\prime\in S^\ominus_s$, $\omega_s(a_{s^\prime})=s^{\prime\prime}$ indicates that when $i$  is in objective state $s$ and thinking about some subjective state $s^\prime$ and a feasible act profile that he imagines is available in that state, $a_{s^\prime}\in A_{s^\prime,s}$, then he is aware that the resulting subjective state actualized is $s^{\prime\prime}$.
Assume that $\omega_s$ meets the same bijectivity conditions with respect to $\Gamma^\ominus_s$ as $\omega$ does with respect to $\Gamma^\oplus$.
	
This brings us to the issue of how objective states and actions correspond to $i$'s awareness of them.
To that end, we assume that $i$'s awareness is a limited but accurate version of reality.
In particular, we  operate from the assumption that, although $i$ is not aware of all the features of reality,  those of which he \textit{is} aware are correct.  	
For each $s,s^\prime\in S^\oplus$, define the surjective \textit{awareness mapping} $r(\cdot|s):S^\oplus\rightarrow S^\ominus_s$ where $s^{\prime\prime}=r(s^\prime|s)$ means that---when $i$ is in objective state $s$---the subjective state $s^{\prime\prime}$  represents his awareness of some other objective state $s^\prime$.

In yet another minor abuse of notation, given an objective history $h_s=(s_0,\ldots,s)$, let $r(h_s)\equiv(r(s_0|s),\ldots,r(s|s))$ denote the history in $\Gamma_s$ to which it corresponds.
Alternatively, given $h\in H^\ominus_s$, $r^{-1}_s(h)\subseteq H^\oplus$ is the diachronic event corresponding to the history $h$ of which $i$ is aware.

The idea is that, typically, the individual is aware of only some subset of features of the objective state of the world at any given moment.
Thus, $r$ projects all the objective states with those features into a single subjective state that includes only those features.
From this perspective, we can think of $r$ as projecting the objective states associated with a particular synchronic event (which would be given by $r^{-1}$) into a single subjective state that represents the individual's awareness of that event.
Importantly, to the individual, this is not an event, but a state of the world as he is aware of it.
The subjective state is an impoverished version of the objective states that project into it: the individual is unaware of the additional features of world that would allow him to refine his ability to think about his situation.

To elaborate on this, suppose $s\in S^\oplus_t$.
Then,  $s^{\prime}=r(s|s)$ means that $s^\prime$ elaborates only the features of the world of which $i$ is aware in $t$ given the actualization of objective state $s$ in that period; it is what $i$ is aware of regarding his current state when that state is $s$.
For example, suppose $s,s^{\prime\prime}\in S^\oplus$ are objective states in which it is raining and sunny outside, respectively.
Further, assume that in both states,  $i$ is aware of a cat meme on his computer, but not the weather. 
Then, $s^{\prime}=r(s|s)\in S^\ominus_s$ is the subjective state in which $i$ is aware of a cat meme on his computer.
Here, it is also the case that $s^{\prime}=r(s^{\prime\prime}|s)$: being in state $s$, $i$ cannot distinguish his present actualized  state in which it is raining from the counterfactual state in which it is sunny.
In the fully-elaborated objective world, $r^{-1}(s^{\prime})$ is the synchronic event ``$i$ is aware of a cat meme''---i.e., all the states in which this is true.
Now, in the present state $s$, $i$ may also be aware that, tomorrow, he will be aware of a new meme---corresponding to a different subjective state $s^{\prime\prime\prime}\in S^\ominus_s$.
His awareness of this alternative subjective state corresponds to a synchronic event $r^{-1}(s^{\prime\prime\prime})$ in reality.
 

We impose the following consistency conditions on $r$:
\begin{enumerate}
	\item Time consistency: if $s^\prime\in S_t\subset S^\oplus$ then $s^{\prime\prime}\in S^\ominus_{t,s}$; e.g., $i$ does not mistake something that happened yesterday with something happening today.
	\item Act independence: for all $s\in S^\oplus$ and $s^\prime\in S^\ominus_s$, $A_{s^\prime,s}\equiv A^n_{s^\prime,s}\times A^i_{s^\prime,s}$; i.e., the set of act profiles includes all combinations of the acts by $n$ and $i$ of which $i$ is aware in state $s$.
	\item No imaginary acts for $i$: if $s^{\prime\prime}=r(s^\prime|s)$, then $A^i_{s^{\prime\prime},s}\subseteq A^i_{s}$; i.e., although $i$ may not be aware of all the feasible acts available to him in a particular state, the acts of which he is aware are truly feasible.
	\item Dynamic consistency: the terminal histories in $\Gamma_s$ correspond to a partition of the terminal histories in $\Gamma^\oplus$. That is, $\{s\in S^\ominus_T|r^{-1}(h_s)\}$ is a partition of $H^\oplus$.
	\item Perfect recall: the individual has perfect recall of his own past acts. Formally, given the objective period $t$ state  $s_t\in S_t$, if $r(h_{s_t})=h_{s^\prime_t}=(s^\prime_0,\ldots,s^\prime_t)$, then for each $s^\prime_j,j>0$ in the sequence $h_{s^\prime_t}$, the action profiles $\omega^{-1}(s^\prime_j)=a^\prime_{j-1}$ and $\omega^{-1}(s_j)=a_{j-1}$ agree on the $i$-act components. That is, if $a^\prime_{j-1}=(a^{n\prime}_{j-1},a^{i\prime}_{j-1})$ and $a_{j-1}=(a^n_{j-1},a^i_{j-1})$, then $a^{i\prime}_{j-1}=a^i_{j-1}$.
\end{enumerate}


%We assume time consistency.
%That is, 
%	
%We also impose certain consistency conditions on $i$'s awareness over time.
%If $h^\oplus_t=(s^\oplus_0,\ldots,s^\oplus_t)$ is the objective history at $t$, then $i$ is aware of the corresponding history as he experienced it, namely, $(s^\ominus_0,\ldots,s^\ominus_t)$ where $s^\ominus_0=r(s^\oplus_0|s^\oplus_0),\ldots, s^\ominus_t=r(s^\oplus_t|s^\oplus_t)$.
%We assume that, at $s^\oplus_t$, $i$ has perfect recall of his own actions corresponding to  $h^\oplus_t$.
%We do not require $i$ to be aware of Nature's acts (though, he may well be so).
%Instead, $i$ may speculate or hypothesize about what Nature's acts are that, in combination with his own, get from $s^\ominus_0$ to $s^\ominus_t$.
	
\paragraph{Example: Brian's Infant, Part 2}	
Picking up the example of Brian's Infant where we left off, let consider the infant's awareness of reality.
Suppose the act profile is $a_{s_0}=((B^\ast,W^\ast),p_1)$.
According to Figure \ref{Diag: p-01}, this brings the world to objective state $s_{1.3}=((B^\ast,W^\ast),p_1)$ in  $t=1$.
Assume that, having decided to solve the toy-choice problem, $i$ becomes aware of which toy is best.
In this state, $i$ can reason about $B$ being the best toy and the counterfactual that $A$ could have been the best toy.
Here, we see that $i$ is also aware that she could have chosen to solve $p_2$, which would have put her in a different state of the world. 
Then, $S^\ominus_0=\{s^\prime_0\}$, such that $P_{s^\prime_0}=P_{s_0}$, and  $S^\ominus_1=\{(A^\ast,p_1),(B^\ast,p_1),(p_2)\}$.\footnote
{
	Keep in mind, the states in $i$'s subjective, state-contingent awareness trees are mathematically distinct from each other and from the objective states. So, we write  $S^\ominus_0=\{s^\prime_0\}$ rather than $S^\ominus_0=\{s_0\}$ even though $P_{s^\prime_0}=P_{s_0}$ because $s^\prime_0$ is not the same object as $s_0$
}

In this state, her awareness tree is $\Gamma^\ominus_{s_{1.3}}$, as depicted in Figure \ref{Diag: p-02}.
In this diagram, the top half shows $\Gamma^\oplus$ tipped on its side. 
The bottom half illustrates $\Gamma^\ominus_{s_{1.3}}$.
Several of the awareness mappings are labelled.
For example, $i$ finds herself in $r(s_{1.3}|s_{1.3})=(B^\ast)$.
Notice that $r^{-1}((B^\ast)|s_{1.3})=\{s_{1.3},s_{1.4}\}$, which corresponds to the objective event, ``$B$ is the best toy and $i$ chooses to solve the indoor-play problem.
The diagram labels several of the other awareness mappings associated with state $S_{1.3}$.

The central take-away from Figure \ref{Diag: p-02} is that $i$ is completely unaware of the TV show choices.
In particular, it is \textit{not} the case that $i$ is \textit{uncertain} about the best TV show.
Rather, according to $\Gamma^\ominus_{s_{1.3}}$, $i$ is simply not thinking about TV shows at all---they are not in her field of awareness and, therefore, are not a factor in her future deliberations.

We can also check the conditions on the awareness mapping. 
Condition 1 is met because each objective state $s\in S_t$ maps to a state in $S^\ominus_t$.
Condition 2 is met because all combinations of the acts of which $i$ is aware are included as act profiles in $\Gamma^\ominus_{s_{1.3}}$.\footnote
{
	The acts of which $i$ is aware are $\{p_1,p_2\}$ for herself and $\{A^\ast,B^\ast\}$ for Nature. The set of act profiles is  $\{A^\ast,B^\ast\}\times \{p_1,p_2\}$. These are all included as branches in $i$'s awareness tree. 
}
Condition 3 is clearly met. 
Condition 4 is met, which can be seen from the fact the terminal states in $S^\ominus$ imply a partition of the terminal states in $S_1$ (and, hence, of $H^\oplus$).
Finally, Condition 5 is met as well: $i$ is able to recall her own act $p_1$, which co-actualized her present state along with Nature's act (of which she is partially aware).

\begin{figure}[h!]
	\centering
	\includegraphics*[page=2,trim = 0in 0in 0in 0in,scale=.6]{Awareness_Diagrams_All}
	\caption{Awareness tree for Brian's Infant when the objective state is $s_{1.3}=((B^\ast,W,p_1)) $.\label{Diag: p-02}}%trim: L B R TxPer
\end{figure}
	
\section{Mental attitudes}
	
\subsection{Uncertainty}
	
In addition to the states about which $i$ is aware, we wish to account for uncertainty. 
Uncertainty is not the same as unawareness.
For example, in the Brian's Infant example, $i$ could be aware that $A^\ast$ and $B^\ast$ are possible yet harbor uncertainty about which is consistent with the true state of the world---and all the while be utterly unaware of the weather.

\subsubsection{Information sets}
For each objective state $s\in S^\oplus$, define the\textit{ information set at subjective state} $s^\prime\in S^\ominus_s$, denoted $I_{s^\prime,s}$,  to be a subset of $S^\ominus_s$.  
Information sets serve to distinguish states about which $i$ is aware but uncertain. 
Information sets are assumed to meet the following conditions:	
\begin{enumerate}
	\setcounter{enumi}{5}
	\item No-delusion: $s^\prime\in I_{s^\prime,s}$.
	\item Introspection: if $s^{\prime\prime}\in I_{s^\prime,s}$, then $I_{s^{\prime\prime},s}=I_{s^\prime,s}$.
	\item Feasible act consistency: For all $s^\prime,s^{\prime\prime}\in S^\ominus_s$, if $A^i_{s^\prime,s}\cap A^i_{s^{\prime\prime},s}\ne \emptyset$ then $A^i_{s^\prime,s}= A^i_{s^{\prime\prime},s}$.
	\item Distinct acts at disjoint information sets: if $s^{\prime\prime}\in S^\ominus_{t,s}$ and $A^i_{s^\prime,s}= A^i_{s^{\prime\prime},s}$, then $I_{s^\prime,s}=I_{s^{\prime\prime},s}$.
	\item Time consistency: if $s^\prime,s^{\prime\prime}\in I_{s^\prime,s}$ and $s^\prime\in S_{t,s}$, then $s^{\prime\prime}\in S_{t,s}$.
\end{enumerate}

Condition 6 is self-explanatory. 
Condition 7 prevents states from being in more than one information set (which would make no sense: if $i$ is uncertain about whether $s^{\prime}$ or $s^{\prime\prime}$ is true and about whether $s^{\prime\prime}$ or $s^{\prime\prime\prime}$ is true, then he should also be uncertain about whether $s^{\prime}$ or $s^{\prime\prime\prime}$ is true).
Condition 8  partitions the states in $S^\ominus_{t,s}$ into equivalence classes according to the acts available to $i$.
Condition 9 implies that $i$ can distinguish between states based upon differences in their feasible acts.
Condition 10, which is similar to Condition 1 for awareness, ensures that all the states in a given information set are possibilities at a specific time $t$.
The idea is that $i$ is always aware of the time and, therefore, never places any weight on the proposition that he is presently in a state occurring at a time different than the one he is in. 


Notice the implication that, for each $s\in S^\oplus$, a state space in a given period, $S_{t,s}$ is partitioned by its information sets.
Moreover, since the time-stamped state spaces $S_{t,s}$ themselves partition $S_s$, the information sets also partition $S_s$.
Therefore, let $\mathcal{I}_s$ denote the collection of all information sets associated with $\Gamma_s$.
This allows us to refer to an arbitrary information set simply as $I\in\mathcal{I}_s$.
Then, because Condition 9 says that all states in the same information set have the same feasible actions (of which $i$ is aware), we can write $A^i_I$ without ambiguity.
	
\paragraph{Example: Brian's Infant, Part 3}	Information sets are illustrated for the Brian's Infant example in Figure \ref{Diag: p-03}. 
In this scenario, $i$ is in objective state $s_{1.3}$ and has the same awareness as in Figure \ref{Diag: p-02}. 
Now, information sets $I$ and $I^\prime$ have been added to illustrate the case in which $i$ is uncertain about whether $A$ or $B$ is the best toy.
These are indicated by the dashed lines connecting states. 
$I$ indicates that $i$ knows she is working on $p_1$ but uncertain about which toy is best.
She is aware that should could have known that she was working on $p_2$ had she chosen that problems (according to $I^\prime$).
	
\begin{figure}[h!]
	\centering
	\includegraphics*[page=3,trim = 0in 6in 6.5in 0in,scale=.8]{Awareness_Diagrams_All}
	\caption{Partially aware infant is also uncertain about which toy is best\label{Diag: p-03}}%trim: L B R TxPer
\end{figure}
	
When an information set is not a singleton (i.e., $i$ is uncertain about something), we assume she has beliefs about which state is true that are represented by a probability distribution on the states in the information set. 
We use $\rho^i$ to indicate these assessments. 
In the preceding example, assuming $s^\oplus_t=(C,A^\ast)$ is understood from the context, $\rho^i_{0.1}(R,A^\ast)$ is the probability $i$ assigns to the possibility that the true state is $(R,A^\ast)$.
	
\subsubsection{Beliefs} \label{sec: beliefs}
Given an objective state $s\in S^\oplus$, let $\Delta(H^\ominus_s)$ denote the set of all probability distributions on the set of subjective terminal histories. 
Then,  $\mu_s\in \Delta(H^\ominus_s)$ $i$ is $i$'s belief about how dynamics play out, with $\mu_s(h)$ indicating the probability $i$ places on terminal history $h\in H^\ominus_s$. 
Because act profiles and states imply events in $H^\ominus_s$ by the terminal histories that pass through them, $\mu_s$ can be used to compute conditional probabilities.
In particular, when $i$ is in subjective state $s^\prime\in S^\ominus_s$, the probability $i$ assigns to $s^\prime$ is  $\mu_s(s^\prime|I_{s^\prime,s})$.
We adopt the convention that if  $\mu_s(I_{s^\prime,s})=0$, then  $\mu_s(s^\prime|I_{s^\prime,s})=0$.

For example, consider $S^\ominus_1$ in Figure \ref{Diag: p-02} in which each state is in its own, singleton, information set.
Then,  $\mu_{s_{1.3}}((B^\ast,p_1)|(B^\ast,p_1))=1$: $i$ knows she is at $(B^\ast,p_1)$.
Instead, suppose the situation is identical except that the information sets are as shown in Figure \ref{Diag: p-03}, with $\mu_{s_{1.3}}$ assigning equal probability to both states in $I$.
Then, $\mu_{s_{1.3}}((A^\ast,p_1)|I)=\mu_{s_{1.3}}((B^\ast,p_1)|I)=0.5$.

%Eventually, we will describe how these beliefs can be constructed from the plans of the individual and the behavior of Nature. 
For now, it is sufficient to understand that they are probability distributions on terminal histories and that they may vary state by objective state.

%		It is important to note that the existence of more than one element in $S_0$ means that individuals may be uncertain about which tree is the objective one and, hence, the true history they have experienced. 
%	If so, they will be uncertain about which state they are in. 
%	In addition, there will be uncertainty about how the future unfolds. 
%	At the moment, we have the objective world starting at $s_0^\ast$ and unfolding in accordance with $\omega$ and the sequence of everyone's act choices. 
%	Since  acts are free choices by individuals, it is possible they are selected randomly (``now, I will decide what to do by flipping a coin'').
%	This includes acts of Nature.
%	All of individual $i$'s speculation with respect to the history, state and unfolding of events is summarized by $\mu_i$.
%	
%	
%	Like in the case of incomplete information, we proceed by introducing probability distributions on state-spaces. For any state space $S \in \mathcal{S}$, let $\Delta(S)$ be the set of probability distributions on $S$. Even though we consider probability distributions on each space $S \in \mathcal{S}$, we can talk about probability of events that, as we just have seen, are defined across spaces. To extend probabilities to events of our lattice structure, let $S_{\mu}$ denote the space on which $\mu$ is a probability measure. Whenever for some event $E \in S$ we have $S_{\mu} \succeq S(E)$ (i.e., the event $E$ can be expressed in space $S_{\mu}$) then we abuse notation slightly and write
%	\begin{equation*}
%		\mu \left( E \right) = \mu \left( E\cap S_{\mu}\right).
%	\end{equation*}
%	If $S(E) \npreceq S_{\mu}$ (i.e, the event $E$ is not expressible in the space $S_{\mu}$ because either $S_{\mu}$ is strictly poorer than $S(E)$ or $S_{\mu}$ and $S(E)$ are incomparable), then we leave $\mu(E)$ undefined.
%	
%	To model an agent's awareness of events and beliefs over events and awareness and beliefs of other groups, we introduce type mappings. Given the preceding paragraph, we see how the belief of an agent at state $\omega \in S$ may be described by a probability distribution over states in a less expressive space $S'$ (i.e., $S \succeq S')$. This would represent an agent who is unaware of the events that can be expressed in $S$ but not in $S'$. These events are ``out of mind'' for him in the sense that he does not even form beliefs about them at $\omega$:  his beliefs are restricted to a space that cannot express these events.
%	
%	More formally, for every agent $i \in N$ there is a \textit{type mapping} $t_{i}: \Omega \longrightarrow \bigcup_{S \in \mathcal{S}} \Delta(S)$. That is, the type mapping of agent $i \in N$ assigns to each state $\omega \in \Omega$ of the lattice a probability distribution over some space. Now a state does not only specify which events affecting value creation may obtain, and which beliefs agents hold over those events, but also which events agents are aware of. Recall that $S_{\mu}$ is the space on which $\mu$ is a probability distribution. Since $t_i(\omega)$ now refers to agent $i$'s probabilistic belief in state $\omega$, we can write $S_{t_i(\omega)}$ as the space on which $t_i(\omega)$ is a probability distribution. $S_{t_i(\omega)}$ represents the \emph{awareness level} of agent $i$ at state $\omega$. This terminology is intuitive because at $\omega$ agent $i$ forms beliefs about \textit{all} events in $S_{t_i(\omega)}$.
%	
%	For a type mapping to make sense, certain properties must be satisfied. The most immediate one is \textit{Confinement:} if $\omega \in S'$ then $t_{i}(\omega )\in \triangle \left( S \right)$ for some $S \preceq S^{\prime}$. That is, the space over which agent $i$ has beliefs in $\omega$ is weakly less expressive than the space contains that $\omega$. Obviously,  a state in a less expressive space cannot describe beliefs over events that can only be expressed in a richer space.  We also impose Introspection, which played a role in our prior discussion of incomplete information: every agent at every state is certain of her beliefs at that state. In AppendixXX, we discuss additional properties that guarantee the consistent fit of beliefs and awareness across different state-spaces and rule out mistakes in information processing.
	% \begin{figure}[h!]	
		% 	\begin{center}
			% 		\includegraphics[scale=.4]{lattice3.jpg}
			% 	\end{center}
		% \caption{Unawareness structure\label{lattice3}}
		% \end{figure}
	
%	It might be helpful to illustrate type mappings with an example. FigureXX depicts the same lattice of spaces as in FiguresXX and XX. In addition, we depict the type mappings for three different groups. At any state in the upmost space $S_{pq}$, the blue agent is aware of $p$ but unaware of $q$. Moreover, she is certain whether or not $p$ depending on whether or not $p$ obtains. This is modeled by her type mapping that assigns probability 1 to state $p$ in every state where $p$ obtains and probability 1 to state $\neg p$ in every state where $\neg p$ obtains. (The blue circles represent the support of her probability distribution that must assign probability 1 to the unique state in the support.) An analogous interpretation applies to the red agent except that she is an expert in $q$. In contrast, the green agent is aware of both $p$ and $q$ but knows nothing with certainty, modeled by her probabilistic beliefs in the upmost space that assigns equal probability to each state in it.\footnote{The example is taken from \citet{Schipper2016} who shows how a generalist (i.e., the green agent) emerges as an entrepreneur and forms a firm made of specialists (i.e., the blue or red agents) in a knowledge-belief and awareness-based theory of the firm using strategic network formations games under incomplete information and unawareness.}
%	
%	Unawareness structures allow us to model an agent's awareness and beliefs about another agent's awareness and beliefs, beliefs about that, and so on. This is because, as in the incomplete information case,  beliefs are over states and states also describe the awareness and beliefs of groups.  Return to FigureXX. At state $pq$ the green agent assigns probability $1$ that the blue group is aware of $p$ but unaware of $q$. Moreover, he assigns probability $1$ to the blue agent believing with probability 1 that the red group is unaware of $p$.\footnote{We  note, it has been shown that under appropriate assumptions on spaces $S \in \mathcal{S}$ and the type mapping, unawareness structures are rich enough to model any higher order beliefs of agents (see the working paper version of \citet{Heifetz2013}).}
%	


\subsection{Desires} \label{sec: desires}
For all $s\in S^\oplus$, define the state-dependent \textit{desire relation}    $D^\ominus_s\subset H^\ominus_s\times H^\ominus_s$ where, $(h,h^{\prime})\in D^\ominus_s$ means that  $i$ in objective state $s$ desires the subjective terminal history $h^{\prime}$ at least as much as the history $h$. 
We use the intuitive notation $h\preceq_s h^{\prime}$, which is defined to mean $(h,h^{\prime})\in D^\ominus_s$. 
We use $\prec_s$ and $\approx_s$ to indicate strict preference and indifference, respectively. 
Assume that each $D^\ominus_s$ is complete (in the sense that any pairs of histories $h,h^{\prime}\in H^\ominus_s$ are comparable) and transitive. 
Then, the desire relation $D^\ominus_s$ can be represented by a numerical function $d^\ominus_s:H^\ominus_s\rightarrow \mathbb{R}$ , where $d^\ominus_s(h)< d^\ominus_s(h^\prime)$ and $d^\ominus_s(h)= d^\ominus_s(h^\prime)$ if and only if  $h\prec_s h^{\prime}$ and $h\approx_s h^{\prime}$, respectively.

Note the implication that desires may change from objective state to objective state.
The objective state provides a fully-featured snapshot of the world, including the status of $i$'s cognition. 
The desire function says that $i$ is capable of comparing the desirability of complete histories---up to his awareness of the world according to his present, objective state.

We wish to impose some consistency between $i$'s desires over the terminal histories as he is aware of them in a given state and his desires over the objective histories underlying them.\footnote
{
	Remember, the histories of which $i$ is aware at any point correspond to collections of objective histories, the latter being the most refined. 
	Although $i$'s awareness limitations coarsen his perception of reality, the consequences of his actions will correspond to the actual unfolding of a specific objective history. 
	This raises the question of how consistent with reality his desires should be.
}
To this end, let $d^\oplus:H^\oplus\rightarrow \mathbb{R}$ be the desire function for $i$ under conditions of perfect awareness (i.e., a situation in which $i$ is aware of all the terminal histories and, hence, can assess each one).
Then, we assume that, for all $s\in S^\oplus,h\in H^\ominus_s$, $d^\ominus_s$ meets the following consistency condition:
\begin{enumerate}
	\setcounter{enumi}{10}
	\item Let 
	\[
		x=\min_{h^\prime\in r^{-1}(h)}d^\oplus(h^\prime),
	\]
	 and  
	 \[
	 	y=\max_{h^\prime\in r^{-1}(h)}d^\oplus(h^\prime).
	 \]
	 Then $d^\ominus_s(h)\equiv\alpha x + (1-\alpha)y$ for some $\alpha\in[0,1]$. 
\end{enumerate}
The interpretation is as follows. 
We imagine that $i$ is equipped with desires ($d^\oplus$) at the beginning of time which would permit him to compare his desires with respect to any two objective histories, \textit{were he aware of $H^\oplus$}.
When he is not perfectly aware, each history of which he is aware corresponds to a diachronic event.
Thus, $d^\oplus$ implies an interval of values $[x,y]$ for each such event.
Then, the structure of $d^\ominus_s$ means that $i$'s desires at lower levels of awareness provide an implicit ranking of the objective intervals determined by   $d^\oplus$.
Notice that the $\alpha$ parameter does not vary by state. 
It is a constant primitive of $i$'s desires through time.\footnote
{
	Allowing for, i.e., $\alpha_s$ could be done, but with a greater number of consistency conditions.
}
That said, because $i$'s awareness may change from objective state $s$ to objective state $s^\prime$, the partitions on $H^\oplus$ implied by $H^\ominus_s$ and $H^\ominus_{s^\prime}$ may vary, thereby implying the possibility of variation in the structure of desires from $d^\ominus_s$ to $d^\ominus_{s^\prime}$. 

Again, $i$'s desires over objective histories under perfect awareness ($d^\oplus$) can be thought of as a cognitive primitive that assigns numerical intervals to the subjective histories under imperfect awareness,
Then, in objective state $s$, the ranking of subjective histories provided by $d^\ominus_s$ is consistent with a ranking function on these intervals. 
This is the key consistency condition. 

%Note also that because the min and max are taken over all $h^\prime\in r^{-1}(h)$, $d^\ominus_s$ is constant on $r^{-1}(s|s)$; i.e., although the desire function is a function on the objective states, it does not vary over those that project into the same awareness state.
  

	
Why consider desires over histories? 
Because we assume individuals care about how they get to an end as well as the end itself. 
To take a canonical example, a homeowner may have a renovated kitchen in mind as the desired end. 
However, even if the kitchen specs are provided in extensive detail (so the owner knows exactly what the end will be), there may be many contractors who can deliver it. 
In this case, assuming there are several contractors from which to choose, each of which identify with a different path with states encoding costs  at each step of the way and the final quality of the work, the owner's choice will be based upon the path (costs) as well as the final state (quality). 
Similarly, an individual sensitive to the time value of money will prefer shorter paths to longer ones, other things equal. 
Or, individuals may value portions of the paths themselves.
For example, even though a student drops out of school (thereby, not completing the degree), he or she may nevertheless value the portion of the education that was completed. 
Our approach allows for special cases in which all these details are elaborated as primitives of the situation. For our discussion, we simply assume preferences are over paths.    


	
	
\subsection{Intentions} \label{sec: intentions}
	
For all $s\in S^\oplus$, define the state-dependent \textit{intention} for the individual as $\gamma_s\subseteq H^\ominus_s$, where $\gamma_s=E$ means that in objective state $s$ individual $i$ intends subjective event $E$. 
We assume that individuals have desires and beliefs in all states, but not necessarily intentions. 
The idea here is that, e.g., in some states Mike intends the end ``Mike has a cup of coffee'' and in others, Mike has yet to form intentions.
We adopt the convention that $\gamma_s=\emptyset$ means that $s$ is a state in which individual $i$ has not formed an intention. 
We highlight that states may be differentiated only by changes in mental attitudes. 
For example, it may be that the only change from $s_t$ to $s_{t+1}$ is $\gamma_{s_t}=\emptyset$ to $\gamma_{s_{t+1}}=E\ne\emptyset$.
This suggests that the interval between time periods may be very short (measured in milliseconds).
	
This raises the question of how an individual moves from being in a state without an intention to one in which the intention is formed. 
Here, we can require an act of commitment to cement the intention. 
That is, if $s_t$ is an objective state in which $i$ does not have an intention, then the set of objective feasible acts, $A^i_{s_t}$, can include an \textit{act to form the intention} (e.g., to ``get a cup of coffee''), which would then take him to a state $s_{t+1}$ in which $\gamma_{s_{t+1}}=E$ where $E$ contains all the states consistent with $i$'s intention (e.g., having a cup of coffee).
	
Summing up, in each objective state, individual $i$'s \textit{mental attitudes} are summarized by a triple denoted $\theta_s\equiv(\mu_s,D_s,\gamma_s)$.
In setting up mental features in this way, we are following a version of the familiar ``type-space'' approach used in game theory \citep[See][]{Harsanyi1967, Mertens1985a} in which $\theta_s$ is $i$'s mental type in objective state $s$. 
	
	
	




\pagebreak
\section{Notation Reference}
Table \ref{Tab: Notation} elaborates all the mathematical objects used in the paper.
\begin{table*}\centering
\ra{1.3}
\begin{tabular}{@{}rll@{}}\toprule
Object                                             & \multicolumn{1}{c}{Description}                       & \multicolumn{1}{c}{Comments}\\ \midrule
%$N\equiv \{1,\dots,n\}$                            & The set of $n$ individuals                            & \\
%$i\in N$                                           & An arbitrary individual                               & $i=0$ is Nature\\
$S^\oplus$                                         & All objective states of the world       & $S^\oplus$ contains all possible states\\
%$S_s$                                       & An arbitrary state in $S^i$                           & $s_t$ the state in period $t$,\\
%$S^\ominus$                   & states of which                & \\
%$\mathcal{S}\equiv \{S^\emptyset,S^\oplus,\ldots,S^\oplus\}$ & Lattice of awareness spaces                           & Maximum is $S^\oplus$ and minimum is $S^\emptyset$\\
%												   &													   & $\mathcal{S}_t,S^i_t$ lattice \& state spaces in $t$\\
%$S^i\succeq S^j$                                   & Individual $i$ is at least as aware as $j$            & \\
%$S\equiv\bigcup_{i\in N}S^i$                  & Union of the individual spaces                        & \\
%$r^{i\rightarrow j}(s^i)$                          & Impoverished version of $s^i$ perceived by $j$        & Only defined if  $S^i\succeq S^j$\\
%$B^{\downarrow}$                                   & A synchronic event $B^{\downarrow}\subseteq S$   & $B^{\downarrow}=\bigcup_{S^j\in \mathcal{S}}\left(r^{0\rightarrow j}\right)^{-1}(B)$, $B\subseteq S^\oplus$\\
$A^i_s$                                           & $i$'s feasible acts in state $s\in S^\oplus$               & $a^i_s\in A^i_s$,  arbitrary act by $i$ in $s$\\
$a_s$                                       & A profile of acts in $s$, $a_s\equiv(a^n_s,a^i_s)$ & \\
$A_s$                                    & All act profiles in $s$                               & \\
%$A_t$                                     & All act profiles at time $t$                          & $A_t\equiv \cup_{s\in S^\oplus_t} A(s)$\\
%$A$                                       & All possible act profiles                             & $A\equiv \cup_{s\in S^\oplus} A(s)$\\
%$\omega(a_t,s^\oplus_t)$                       & State actualized in $t+1$ from $s^\oplus_t$ given $a_t$    & e.g., $s^\oplus_{t+1}=\omega(a_t,s^\oplus_t)$\\
$h_s$                                & History at $s\in S^\oplus$                                    &  \\
$\mathcal{H}_t$                                    & Set of all subsets of histories at $t$                & \\
\bottomrule
\end{tabular}
\caption{Notation Reference (NEEDS TO BE UPDATED AND EXPANDED)}\label{Tab: Notation}
\end{table*}
\pagebreak

\section{Four-Phase Model of Action}

With the previous setup in place, we can now elaborate the model discussed in the Introduction.
The phases are: 1) Problem Selection; 2) Deliberation; 3) Planning; and 4) Acting.
Each phase requires completion of the preceding phases before it can begin.
As we elaborate the phases, we also add certain consistency conditions which are pertinent to them.


\begin{figure}[h!]
	\centering
	\includegraphics*[page=8,trim = 0in 3in 0in 0in,scale=.65]{Awareness_Diagrams_All}
	\caption{Overview of states and decision phases.\label{Diag: p-08}}%trim: L B R TxPer
\end{figure}

The objective tree $\Gamma^\oplus$ elaborates all the ways in which the world can evolve.
Consistent with $\Gamma^\oplus$, $H^\oplus$ contains all the terminal histories, each of which elaborates a particular evolution from the beginning state, $s_0$, to some terminal state $T$ periods later.
The elements of $A^\oplus$ describe the feasible act profiles at every state.
The act mapping $\omega$ connects the act profiles available in one state lead to the new states in the following period which they actualize.
See Figure \ref{Diag: p-08} for a summary.



\paragraph{Brian's Infant, Part IV}
Figure \ref{Diag: p-04} illustrates $\Gamma^\oplus$ for the Brian's Infant example. 
Here, we introduce some futher details.
First, to illustrate the four phases of the decision-act process, we extend the tree through $T=4$.
Next, we add the details of what could happen at each phase.
At $t=0$, $i$ decides whether to solve $p_1$ or $p_2$.
At the same time, Nature fixes the best toy and the best TV show. 
For now, we assume Nature does not act until the penultimate period.

\begin{figure}[h!]
	\centering
	\includegraphics*[page=4,trim = 0in 0in 0in 0in,scale=.65]{Awareness_Diagrams_All}
	\caption{Objective tree spanning four periods.\label{Diag: p-04}}%trim: L B R TxPer
\end{figure}

In $t=1$, $i$ deliberates and commits to an intention. 
The intention is a commitment to influence events such that a particular diachronic event is actualized.
It is a goal. 
Here, ff $i$ chooses $p_1$, then her intentions can be either to get $A$ or to get $B$.
Alternatively, if she chooses $p_2$, then her intentions can be to either to watch $W$ or  $C$.

With an intention in place at the start of $t=2$, $i$ then formulates a plan by which to accomplish the intention. 
Here, we assume the plan under $p_1$ is to approach and grab the intended toy. 
Under  $p_2$, the plan is to pick up the TV remote and click to the intended show.
Typically, state-contingent plans will be more elaborate, but in this simple example, the plan is a single-step.

With the plan in place at the start of $t=3$, $i$ acts according to the plan.
At this point, we assume Nature acts once again to determine the success of $i$'s act with respect to the intention commitment at $t=1$.
For example, failure could involve $i$ dropping the toy or Brian entering the room, sweeping up the toys, and placing them into a toy box out of $i$'s reach.

To avoid clutter, Figure \ref{Diag: p-04} does not illustrate every history in $\Gamma^\oplus$.
Still, there should be enough detail elaborated in the diagram to infer the entire structure of the objective tree.
The number of states are expanding through time, from a single state at $t=0$ to thirty-two at $t=4$.

Finally, let us quantify $d\oplus$ in this example as follows:
\begin{enumerate}
	\item $d^\oplus(h)=100$ if $h$ includes successfully obtaining the best toy;
	\item $d^\oplus(h)=20$ if $h$ includes successfully obtaining the second best toy;
	\item $d^\oplus(h)=50$ if $h$ includes successfully choosing the best TV show;
	\item $d^\oplus(h)=10$ if $h$ includes successfully choosing the best TV show;
	\item $d^\oplus(h)=-10$ if $h$ includes failing to achieve any of the above.
\end{enumerate}
Since the states in $S_4$ each correspond to a terminal history, we can write, e.g., $d^\oplus(s_{4.1})$ without ambiguity.
Thus, $d^\oplus(s_{4.1})=100$, $d^\oplus(s_{4.2})=-10$,  $d^\oplus(s_{4.7})=25$, $d^\oplus(s_{4.31})=50$, and so on.


\subsection{Problem Selection}

As discussed above, at the beginning of time, in objective state $s_0$, the individual may select from a set of mutually exclusive act-problems. 
The mutual exclusivity may be due to something inherent in the problems themselves (for example, Brian's infant may be faced with a decision about how to play indoors or how to play outdoors---these are inherently mutually exclusive), or they may be mutually exclusive due to the cognitive capacity constraints (for example, Brian's infant can play with a toy while watching TV, but only has the capacity to deliberate which toy is best or which show is best, but not both simultaneously).

For each objective state, $s$, the act-problems that are available for deliberation are given by $P_s$.
At $s_0$, choosing an element in $P_{s_0}$ is the \textit{only} act available to $i$.
Individual $i$ assesses which problem to solve at $s_0$ based upon desires $D_{s_0}$ and beliefs $\mu_{s_0}$.
In any future state, the individual may stop what he is doing and select a new act-problem to solve.

For now, we assume that $i$ is aware of all the possible act-problems at $s_0$.
However, we also assume that $i$ has only a vague sense of the dynamics involved with each of these problems.
Specifically, let $\Gamma_{s_0}$ be a tree with $|P_{s_0}|$, $T$-length branches (one for each problem in $P_{s_0}$).\footnote
{
	So, $i$ is comparing the consequences of solving each problem over an equal time horizon.
}
In this state, $i$ consults his desires and chooses a problem that maximizes $d_{s_0}$.
\textit{This choice of an act-problem is the output of of the Problem Selection phase}.

\paragraph{Brian's Infant, Part V}
Figure \ref{Diag: p-05} illustrates  a partial illustration of the situation in $s_0$ (shown are periods $t=0,1$).
The purpose of this diagram is to show how the state projections work.
The objective tree $\Gamma^\oplus$ is shown on the top half of the diagram which accounts for all the feasible acts by Nature and $i$.
The bottom half illustrates the corresponding parts of $i$'s awareness tree $\Gamma_{s_0}$
Here, $i$ is aware of both act-problems that are available to her at the beginning of the decision process.
However, at this early stage, she is only vaguely aware of what each of these problems entail.
She has not analyzed either one in deep detail.
Therefore, as she anticipates the unfolding of events, she is only aware of the broad histories consistent with ``solving $p_1$'' and ``solving $p_2$.''

\begin{figure}[h!]
	\centering
	\includegraphics*[page=5,trim = 0in 1in 0in 0in,scale=.6]{Awareness_Diagrams_All}
	\caption{Partial awareness tree for Brian's Infant at the beginning of time $s_{0}$.\label{Diag: p-05}}%trim: L B R TxPer
\end{figure}

The complete awareness tree is shown in Figure \ref{Diag: p-06}.
Here, the awareness states are labeled with primes to distinguish them from their objective counterparts.
What $i$ envisions as she contemplates which problem to tackle is a sequence in which she chooses either to play with the best toy or to watch the best show.
She knows the four phases required to get to an action.
However, she is not thinking about the details of Nature's act at $t=0$, nor is she aware of her feasible acts at each state.
She does realize that, at the conclusion of her efforts, she may succeed or fail depending upon events outside of her control.

\begin{figure}[h!]
	\centering
	\includegraphics*[page=6,trim = 0in .5in 5in 0in,scale=.7]{Awareness_Diagrams_All}
	\caption{Complete awareness tree for Brian's Infant at the beginning of time $s_{0}$.\label{Diag: p-06}}%trim: L B R TxPer
\end{figure}

Notice that the four histories of which she is aware each correspond to eight distinct histories in reality ($4\times 8=32$).
For example, the terminal synchronic event associated with $s_{4.1}^\prime$ is
\[ 
	r^{-1}_{s_0}(s_{4.1}^\prime)=\{s_{4.1},s_{4.3},s_{4.5},s_{4.7},s_{4.9},s_{4.11},s_{4.13},s_{4.15}\}.
\]
These correspond to the diachronic event ``the infant chose  to solve $p_1$ and succeeds in her intended outcome.''
In terms of payoff values, this event includes two kinds of histories: i) those that include success in obtaining the best toy ($d^\oplus(h)=100$); and ii) those that include success in obtaining the second best toy ($d^\oplus(h)=20$).
With respect to Item (ii), note that at $t=1$ the infant is free to choose either toy as her intended plaything, even though she knows which toy is, in fact, best.
Notice that intending second best \textit{is} a feasible act at that point (even if her desires motivate her to intend otherwise).

%This means that the $\alpha$ consistency parameter discussed above is relevant (at both $s^\prime_{1.1}$ and $s^\prime_{1.2})$).
In the  $p_1$/success event, the min value is $20$ and max value is $100$, whereas in the corresponding $p_2$ event, the min value is $10$ and the max value is $50$.
Suppose $\alpha=0.5$. 
Then, the terminal history through $s^\prime_{4.1}$ in Figure \ref{Diag: p-06} has value $d^\ominus_{s_0}(s^\prime_{4.1})=60$ and the one through  $s^\prime_{4.3}$  has value $d^\ominus_{s_0}(s^\prime_{4.3})=30$.
The fail histories, terminating in $s^\prime_{4.2}$ and $s^\prime_{4.4}$,  both have value of $-10$.


To choose between $p_1$ and $p_2$, $i$'s beliefs must come into play.
Specifically, $i$ must assess the probability of success or failure associated with each problem. 
Suppose that in state $s_0$, $i$  believes the probability of success is 0.5 for both problems. 
Then, the expected value of choosing $p_1$ is $0.5*60-0.5*10=25$ and the expected value of choosing $p_2$ is $10$.
Therefore, $i$ chooses $p_1$.


\subsection{Deliberation}
The Deliberation phase begins in $t=1$ in objective state $s_1=\omega(a_{s_0})\in S_1$.
During the time interval during which the state of the world evolves from  $s_0$ to $s_1$, the individual shifts his awareness to support a deliberation on how to resolve his chosen act-problem. 
Typically, the awareness tree in $t=1$ will refine certain aspects of the original awareness tree in $t=0$ and coarsen others.
In the new awareness tree, $i$ is aware of the history of his own moves.
He considers the terminal histories that are reachable from his present state and, based upon his preferences and beliefs, forms a concrete intention.

Intentions take the form $\gamma_{s_1}\subset H^\ominus_{s_1}$.
\textit{An intention is the output of the Deliberation phase}.
Here, it is worth reminding ourselves that the awareness state in which $i$ finds himself is given by $s^\prime_1=r(s_1|s_1)$.
We assume the intention must be subjectively feasible.
That is, the subjective histories included in $\gamma_{s_1}$ must all pass through $s^\prime_1$.\footnote
{
	In most cases, the intention will be to effect a single terminal awareness history; i.e., $\gamma_{s_1}=\{h\}, h\in H^\ominus_{s_1}$. 
	However, this is not required. 
	For example, the intention ``Mike obtains a fresh cup of coffee,'' may be a diachronic event that includes many states (e.g., in which Mike's work being complete may or may not be true).
}
 

\paragraph{Brian's Infant, Part VI}
Assume that the world evolves according to Figure \ref{Diag: p-04}; i.e., from objective states $s_0$ to $s_{1.3}$, where $s_{1.3}$ is actualized by the action profile $a_{s_0}=((B^\ast,W^\ast),p_1)$
This shift in awareness causes $i$ to have a more refined sense of the opportunities and issues associated with $p_1$.
This is illustrated by the evolution from the awareness situation depicted in Figure \ref{Diag: p-06} to the one depicted in Figure \ref{Diag: p-07}. 
In the latter, $i$ is aware that $B$ is the best toy, that $A$ could have been the best toy, and that she could have embarked on a solution to $p_2$ instead of $p_1$. 
She is not thinking about anything having to do with which show to watch.

\begin{figure}[h!]
	\centering
	\includegraphics*[page=7,trim = 3in 0in 1in 0in,scale=.7]{Awareness_Diagrams_All}
	\caption{Complete awareness tree for Brian's Infant at state $s_{1.3}=((B^\ast,W^\ast),p_1)$.\label{Diag: p-07}}%trim: L B R TxPer
\end{figure}

There are a number of items of which to take notice at this point.
First, referring to Figure \ref{Diag: p-07}, in $\Gamma_{s_{1.3}} $, $i$ is aware of being in state $s^{\prime\prime}_{1.2}$.
This state is the projection of the state  $s_{1.3}$ depicted in Figure \ref{Diag: p-04}.
As such, $i$ is also aware of the subjective history that leads to $s^{\prime\prime}_{1.2}$.
From this point, the feasible histories are the ones terminating in $s^{\prime\prime}_{4.1}$ and $s^{\prime\prime}_{4.2}$.
Clearly, the goal is to succeed at obtaining the best toy.
Therefore, the intention going into period $t=2$ is $\gamma_{s_{1.3}}=\{s^{\prime\prime}_{4.1}\}$.

As $i$ contemplates her own future awareness, she expects her future self to be aware that $p_2$ is also a problem that could be solved.
That is, although it is not her intention to switch problems before the success of failure of $\gamma_{s_{1.3}}$, she is aware that the option to abandon her present act-problem in favor of $p_2$ is always present. 
This option is shown in Figure \ref{Diag: p-07} by the history that terminates in  $s^{\prime\prime}_{4.3}$.
Finally, although she is presently aware that Nature could have chosen $A$ as the best toy (counterfactual state $S^{\prime\prime}_{1.1}$),
she does not imagine that she will be thinking about that going forward. 

\clearpage


\subsection{Planing}
Others have suggested that, in addition to helping us think through how to attain a desired end, plans serve the additional function of unencumbering the mind of some portion of its cognitive load. 
We agree and incorporate this aspect of planning explicitly into our analysis.

We write $\sigma^j_s(a^j_s)$ to indicate the probability that act $a^j_{s}$ is selected at $s$.
The use of probability distributions over feasible acts provides a nice level of generality.
We note that, typically, individuals make action choices with certainty: e.g., $\sigma^i_s(a^i_s) =1$ for some $a^i_s\in A^i_s$.
However, this setup also allows for situations in, e.g., $i$ comes to a decision point and decides to ``flip a coin'' to determine which act to choose.

The plan is intentionally developed by $i$ to achieve an intention consistent with his desires and beliefs.
Notice that, in particular, $\sigma^i_{s^\prime,s}$ specifies an act for $i$ at $I_{s^\prime,s}$ where $s^\prime=r(s|s)$ is the subjective state into which  $s$ projects when $i$ is in $s$.
Therefore,  we write $\sigma^i_s$ to indicate the act $i$ will take at the objective state $s$ as implied by his subjective plan.
Finally, note the difference between an agent randomizing over acts versus being uncertain about where in the tree he is. 
Even when $i$ is certain about his state (i.e., is at a singleton information set), he can still choose to randomize over his feasible acts.

Although we do not assume Nature is similarly intentional, we can nevertheless describe her behavior in the same way. 
That is, Nature's ``plan'' is also a list
\[
\sigma^n\in\Sigma^n\equiv \prod_{s\in S^\oplus}\Delta(A^n_s).
\]
Whereas, we imagine individuals typically choose their acts with certainty, Nature's choices almost surely involve some measure of randomness.
At the same time, Nature is never ``uncertain'' about where she is in $\Gamma^\oplus$---all her information sets are singletons.
Therefore, Nature's plan is a description of her behavior at every objective state.
At state $s$, $\sigma^n_s\in (A^n_s)$ is the random act Nature takes at $s$.
%
%	
\subsection{Action}


%	\subsection{Consistency conditions\label{sec:consistencies}}
%	
%	Having structured the objects of interest, we now explore various conditions required to impose the regularities between the various mental attitudes and between those attitudes and the external world that are appropriate to a rational human being. 
%	
%	\paragraph{Reality Alignment\label{para: reality alignment}} 
%	Beginning with the latter, our setup allows individuals to believe (place positive probability on) things that are not objectively true. 
%	However, it is difficult to square rationality with someone whose beliefs are completely divorced from reality. 
%	Therefore, we assume beliefs align with reality at least to some extent.
%	\begin{condition}[Grain of Truth]\label{cond:grain of truth}
%		For all $i\in N$, $s_t\in S$, $\mu_i^s(h^\ast_t)>0$.
%	\end{condition}
%	\noindent That is, rational individuals do not rule out the true state of affairs. 
%	This implies that, although an indivual's beliefs about an event may be wildly inaccurate, that belief is not completely irrational: i.e., for all $W\in \mathcal{H}$ such that $\mu_i^s(W)>0$, $h^\ast_t\in W$. 
%	Going in the other direction, for all $h^\ast_t\in H^\ast$, there exists some $W\in \mathcal{H}$ such that $\mu_i^s(W)>0$.
%	This condition is not without controversy as it does rule out situations in which an individual is surprised by being confronted with a state of affairs he or she had previously thought impossible.
%	There are formal approaches to dealing with such situations.
%	For now, however, we sidestep such issues.
%	
%	\paragraph{Learning\label{para: learning}}
%	We can also think of consistencies implied by learning. 
%	Even with the Grain of Truth Condition in place, our setup presently allows a person's beliefs through time to be completely inconsistent in all ways except $\mu_i^s(h^\ast_t)>0$. 
%	For example, suppose $X,Y\in \mathcal{H}$ and $\mu_i^{s_t}(X)=1$ and $\mu_i^{s_{t+1}}(Y)=1$ ($X$ and $Y$ contain all the states $i$ believes are possible in periods $t$ and $t+1$, respectively). 
%	Then, even if $X$ and $Y$ are quite large, there is nothing in the setup preventing $X\cap Y= h^\ast_{t+1}$; i.e., the \textit{only} consistency from period to period is belief in the possiblity of the objectively true history.
%	Such situations seem inconsistent with any reasonable concept of learning. 
%	The following condition is a notion of learning that admits a wide range of learning models.
%	For example, Baysian updating is consistent with this (though, by no means requred).
%	\begin{condition}[Weak Learning]\label{cond: weak learning}
%		Let $X,Y\in \mathcal{H}$. For all $i\in N$, $s_t,s_x\in S, x>t$, if $\mu_i^{s_t}(X)=1$ and $\mu_i^{s_x}(Y)=1$, then $Y\subseteq X$.
%	\end{condition}
%	\noindent Notice that learning is, indeed, weak in the sense that one may never learn anything ($Y=X$ through time).
%	However, we imagine that as individuals experience the world, their grasp of it becomes more refined. 
%	Again, this condition is also not without controversy since it seems to rule out ``conversion'' experiences in which an individual shifts from one worldview to another, apparently inconsistent worldview.
%	Whether or not such experiences are, in fact, inconsistent with Condition \ref{cond: weak learning} we leave for another discussion.
%	
%	\paragraph{Introspection\label{para: introspecton}} 
%	It seems reasonable to assume that an individual knows his or her own mental features (but may be uncertain of those of others). 
%	For example, being certain of one's own beliefs rules out some peculiar mistakes in information processing (e.g., \citet{Geanakoplos1989}, \citet{Samet1990}). 
%	As described above, the probability distribution representing an individual’s beliefs in may vary by state.  
%	Introspection entails that, at any given state, the agent's belief assigns probability 1 to the set of states in which he has the same belief as in that state. Formally, 
%	\begin{condition}[Introspection]\label{cond: introspection}
%		For each agent $i \in N$ and  state $ s \in S$, the agent's belief at $s$, $\mu_i^s$, assigns probability 1 to the set of states in which  $i$ has precisely these beliefs: 
%		$\mu_i^s(\{s^\prime \in S \mid \mu_i^{s^\prime} = \mu_i^s\})=1$. 
%	\end{condition}
%	
%	\paragraph{Ordering of desires \label{para: desire ordering}} 
%	It is also typical to add some structure to desires, namely that they be a partially ordered. 
%	Formally, for all $i\in N$, $\preceq_i$ is a partial order relation on the set of paths, $P$; i.e., the following conditions hold for all paths in $G^n$:
%	\begin{enumerate}
%		\item $\forall  p^\prime\in S, (p^\prime,p^\prime)\in D(p)$: the relation ip reflexive,
%		\item $\forall  p^\prime,p^{\prime\prime}\in p,(p^\prime,p^{\prime\prime})\in D(p)\wedge (p^{\prime\prime},p^\prime)\in D(p)\Rightarrow p^\prime=p^{\prime\prime}$: the relation ip antipymmetric,
%		\item $\forall  p^\prime, p^{\prime\prime}, p^{\prime\prime\prime}\in p, (p^\prime,p^{\prime\prime})\in D(p)\wedge (p^{\prime\prime},p^{\prime\prime\prime})\in D(p)\Rightarrow  (p^{\prime},p^{\prime\prime\prime})\in D(p)$: the relation is transitive.
%	\end{enumerate} 
%	These conditions simply assume that there is a certain degree of consistency in an individual's desires over states. 
%	
%	\paragraph{Intentions\label{para: intentons}} 
%	An intention differs from both beliefs and desires in that this mental attitude implies the individual possessing it has made a commitment to take action toward a desired end. 
%	The desired end is an event, such as ``Mike buys a cup of coffee,'' which may be actualized by a large number of states of the world; e.g., buying at McDonalds, or at Starbucks, or alone, or with friends, or while believing the dark roast is probably sold out. Thus, in state $s$, the object of individual $i$'s intention is an event in $\mathcal{S}$. 
%	It is not enough for an individual to simply intend some outcome. 
%	Rather, we assume that at the time an intention is formed, it is coupled with a concrete plan of action designed to achieve the desired end. 
%	
%	To formalize this, for each individual $i$, define an \textit{action plan} as a function $\sigma_i:S\rightarrow A$ where $\sigma_i(s)=a_i\in A_i(s)$ indicates that when individual $i$ arrives at state $s$ she selects an act $a_i$ from the set of acts $A_i(s)$ available at that state. 
%	Since every state has a single history leading to it, action plans may be history-contingent.
%	Notice that, as defined, the action plan indicates what act the individual will implement at every state. 
%	Of course, we do not expect the individual to have thought through a contingency plan for every state in the state space. 
%	Rather, we impose a means-ends consistency condition on $\sigma_i$ that joins the action plan to the intention.
%	
%	\begin{condition}[Weak Means-Ends Consistency]\label{cond:weak M-E}
%		Suppose individual $i$'s intention is given by $\gamma_i(s)=X\in\mathcal{S}$. 
%		Let $P^s_X\subset P$ denote all the paths in $G^n$ that begin at $s$ and terminate in $X$. 
%		Then $\sigma_i$ is said to be \textit{weak means-ends consistent with $\gamma_i(s)$} if at no state $s^\prime$ along any path in $P^s_X$ does $\sigma_i^{s^\prime}$ force actualization of a state $s^{\prime\prime}$ that is not on any path in $P^s_X$. 
%		By ``force'' we mean that $\sigma_i^{s^\prime}$ indicates an act that actualizes some state outside of $P^s_X$ regardless of the acts of all the other individuals and Nature. 
%	\end{condition}
%	
%	\begin{condition}[Strong Means-Ends Consistency]\label{cond:strong M-E}
%		Suppose individual $i$'s intention is given by $\gamma_i(s)=X\in\mathcal{S}$. 
%		Let $P^s_X\subset P$ denote all the paths in $G^n$ that begin at $s$ and terminate in $X$. 
%		Then $\sigma_i$ is said to be \textit{strong means-ends consistent with $\gamma_i(s)$} if at every state $s^\prime$ along any path in $P^s_X$,  $\sigma_i^{s^\prime}$ forces actualization of a state $s^{\prime\prime}$ that continues along a path in $P^s_X$. 
%		By ``force'' we mean that $\sigma_i^{s^\prime}$ indicates an act that actualizes some state on a path in $P^s_X$ regardless of the acts of all the other individuals and Nature. 
%	\end{condition}
%	
%	In other words, Condition \ref{cond:weak M-E} says that the individual's plan never has him unilaterally driving the world to a state from which the intended event cannot be reached. 
%	When this condition is met, it may nevertheless be the case that the world is driven to such a state. 
%	However, this will need to be the result of the acts of others and/or Nature and nothing to do with the acts of individual $i$. 
%	The strong form, Condition \ref{cond:strong M-E}, says that individual $i$ has a plan of action by which he can gaurantee his intended even regardless of what anyone else does.
%	There is another case which is this: no matter what $i$ does, the intended $X$ will happen. 
%	In this case, I do not think we would properly call $X$ intention. 
%	
%	We also need some rationality conditions that tie the preferences over paths to the action plan. 
%	This is subtle because paths are determined by the entire act profile (i.e., and not just the acts of $i$. 
%	So, how do you tie in preferences. One possiblity is to use $i$'s may have beliefs about what the other agents are going to do (remember all of this would be encoded in the states) and, based upon this, choose an action plan that implements the most preferred path possible given the plans of the others. 
%	This would then tie beliefs, desires, intentions and plans of action together. 
%	
%	
%	[STOP HERE]
%	
\pagebreak
	\bibliography{library}
	
\end{document}

pdflatex: --aux--directory=build
bibtex: build/% -include-directory=build